"""
LoRAè®­ç»ƒè´¨é‡è¯Šæ–­è„šæœ¬
ç”¨äºæ£€æŸ¥ç¼“å­˜è´¨é‡ã€æ•°æ®å®Œæ•´æ€§å’Œè®­ç»ƒçŠ¶æ€
"""

import os
import torch
import json
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
from tqdm import tqdm

# é…ç½®è·¯å¾„
DATA_DIR = "/content/drive/MyDrive/VisDrone2019-YOLO/VisDrone2019-YOLO-renamed/"
CACHE_DIR = os.path.join(DATA_DIR, "cached_latents")
CHECKPOINT_DIR = "/content/drive/MyDrive/VisDrone2019-YOLO/checkpoints"
OUTPUT_DIR = "/content/drive/MyDrive/VisDrone2019-YOLO/trained_lora_controlnet/"


def check_cache_quality():
    """æ£€æŸ¥latentç¼“å­˜çš„è´¨é‡"""
    print("ğŸ” æ£€æŸ¥Latentç¼“å­˜è´¨é‡...")

    if not os.path.exists(CACHE_DIR):
        print("âŒ ç¼“å­˜ç›®å½•ä¸å­˜åœ¨")
        return False

    cache_files = [f for f in os.listdir(CACHE_DIR) if f.endswith('.pt')]
    print(f"ğŸ“Š ç¼“å­˜æ–‡ä»¶æ•°é‡: {len(cache_files)}")

    # æ£€æŸ¥æ–‡ä»¶å¤§å°åˆ†å¸ƒ
    file_sizes = []
    corrupted_files = []
    valid_samples = []

    print("ğŸ“ åˆ†æç¼“å­˜æ–‡ä»¶å¤§å°...")
    for i, file in enumerate(tqdm(cache_files[:100], desc="æ£€æŸ¥ç¼“å­˜")):  # æ£€æŸ¥å‰100ä¸ª
        file_path = os.path.join(CACHE_DIR, file)
        size = os.path.getsize(file_path)
        file_sizes.append(size)

        # å°è¯•åŠ è½½æ–‡ä»¶
        try:
            latent = torch.load(file_path, map_location="cpu")
            if latent is not None and latent.numel() > 0:
                valid_samples.append({
                    'file': file,
                    'size': size,
                    'shape': latent.shape,
                    'mean': latent.mean().item(),
                    'std': latent.std().item(),
                    'min': latent.min().item(),
                    'max': latent.max().item()
                })
            else:
                corrupted_files.append(file)
        except Exception as e:
            corrupted_files.append(file)
            print(f"âš ï¸ æŸåçš„ç¼“å­˜æ–‡ä»¶: {file}, é”™è¯¯: {e}")

    # ç»Ÿè®¡åˆ†æ
    file_sizes = np.array(file_sizes)
    print(f"\nğŸ“ˆ ç¼“å­˜æ–‡ä»¶å¤§å°ç»Ÿè®¡:")
    print(f"  å¹³å‡å¤§å°: {file_sizes.mean():.0f} bytes")
    print(f"  æœ€å°å¤§å°: {file_sizes.min()} bytes")
    print(f"  æœ€å¤§å¤§å°: {file_sizes.max()} bytes")
    print(f"  æ ‡å‡†å·®: {file_sizes.std():.0f} bytes")

    # æ£€æŸ¥å¼‚å¸¸æ–‡ä»¶
    small_files = (file_sizes < 1000).sum()  # å°äº1KBçš„æ–‡ä»¶
    print(f"âš ï¸ å¼‚å¸¸å°æ–‡ä»¶(<1KB): {small_files}")
    print(f"âŒ æŸåæ–‡ä»¶: {len(corrupted_files)}")
    print(f"âœ… æœ‰æ•ˆæ–‡ä»¶: {len(valid_samples)}")

    # åˆ†ææœ‰æ•ˆæ ·æœ¬çš„latentç»Ÿè®¡
    if valid_samples:
        print(f"\nğŸ“Š Latentæ•°æ®ç»Ÿè®¡ (åŸºäº{len(valid_samples)}ä¸ªæ ·æœ¬):")
        means = [s['mean'] for s in valid_samples]
        stds = [s['std'] for s in valid_samples]

        print(f"  Latentå‡å€¼èŒƒå›´: {min(means):.4f} ~ {max(means):.4f}")
        print(f"  Latentæ ‡å‡†å·®èŒƒå›´: {min(stds):.4f} ~ {max(stds):.4f}")

        # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å€¼
        mean_std = np.std(means)
        std_std = np.std(stds)
        print(f"  å‡å€¼çš„æ ‡å‡†å·®: {mean_std:.4f} (åº”è¯¥<0.5)")
        print(f"  æ ‡å‡†å·®çš„æ ‡å‡†å·®: {std_std:.4f} (åº”è¯¥<0.5)")

        # åˆ¤æ–­è´¨é‡
        if small_files > 10 or len(corrupted_files) > 5:
            print("âŒ ç¼“å­˜è´¨é‡å·®ï¼šå»ºè®®é‡æ–°ç¼“å­˜")
            return False
        elif mean_std > 0.5 or std_std > 0.5:
            print("âš ï¸ ç¼“å­˜æ•°æ®å¼‚å¸¸ï¼šå¯èƒ½æœ‰æ•°æ®ä¸ä¸€è‡´")
            return False
        else:
            print("âœ… ç¼“å­˜è´¨é‡è‰¯å¥½")
            return True
    else:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„ç¼“å­˜æ–‡ä»¶")
        return False


def check_data_integrity():
    """æ£€æŸ¥åŸå§‹æ•°æ®å®Œæ•´æ€§"""
    print("\nğŸ” æ£€æŸ¥åŸå§‹æ•°æ®å®Œæ•´æ€§...")

    # æ£€æŸ¥promptæ–‡ä»¶
    prompt_file = os.path.join(DATA_DIR, "prompt.jsonl")
    if not os.path.exists(prompt_file):
        print("âŒ prompt.jsonlæ–‡ä»¶ä¸å­˜åœ¨")
        return False

    # è¯»å–promptæ•°æ®
    try:
        with open(prompt_file, "r") as f:
            entries = [json.loads(line) for line in f]
        print(f"ğŸ“„ Promptæ¡ç›®æ•°é‡: {len(entries)}")
    except Exception as e:
        print(f"âŒ è¯»å–promptæ–‡ä»¶å¤±è´¥: {e}")
        return False

    # æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶å­˜åœ¨æ€§
    missing_images = []
    existing_images = []

    print("ğŸ–¼ï¸ æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶å­˜åœ¨æ€§...")
    for i, entry in enumerate(tqdm(entries[:200], desc="æ£€æŸ¥å›¾ç‰‡")):  # æ£€æŸ¥å‰200ä¸ª
        image_path = os.path.join(DATA_DIR, entry["image"])
        if os.path.exists(image_path):
            try:
                # å°è¯•æ‰“å¼€å›¾ç‰‡
                img = Image.open(image_path)
                existing_images.append(entry["image"])
                img.close()
            except Exception as e:
                missing_images.append(f"{entry['image']} (æŸå: {e})")
        else:
            missing_images.append(entry["image"])

    print(f"âœ… æœ‰æ•ˆå›¾ç‰‡: {len(existing_images)}")
    print(f"âŒ ç¼ºå¤±/æŸåå›¾ç‰‡: {len(missing_images)}")

    if len(missing_images) > 10:
        print("âš ï¸ ç¼ºå¤±å›¾ç‰‡è¿‡å¤šï¼Œå¯èƒ½å½±å“è®­ç»ƒè´¨é‡")
        print("å‰10ä¸ªç¼ºå¤±å›¾ç‰‡:")
        for img in missing_images[:10]:
            print(f"  - {img}")
        return False
    else:
        print("âœ… æ•°æ®å®Œæ•´æ€§è‰¯å¥½")
        return True


def check_training_progress():
    """æ£€æŸ¥è®­ç»ƒè¿›åº¦å’Œæ¨¡å‹çŠ¶æ€"""
    print("\nğŸ” æ£€æŸ¥è®­ç»ƒè¿›åº¦...")

    # æ£€æŸ¥æ˜¯å¦æœ‰ä¿å­˜çš„æ¨¡å‹
    checkpoints = []
    if os.path.exists(CHECKPOINT_DIR):
        for item in os.listdir(CHECKPOINT_DIR):
            if item.startswith("balanced_checkpoint_epoch_"):
                epoch_num = item.split("_")[-1]
                checkpoints.append(int(epoch_num))

    if checkpoints:
        latest_epoch = max(checkpoints)
        print(f"ğŸ“ æ‰¾åˆ°æ£€æŸ¥ç‚¹: epoch {sorted(checkpoints)}")
        print(f"ğŸ¯ æœ€æ–°æ£€æŸ¥ç‚¹: epoch {latest_epoch}")

        # æ£€æŸ¥æœ€æ–°æ£€æŸ¥ç‚¹
        latest_checkpoint = os.path.join(CHECKPOINT_DIR, f"balanced_checkpoint_epoch_{latest_epoch}")
        lora_weights_path = os.path.join(latest_checkpoint, "pytorch_lora_weights.bin")

        if os.path.exists(lora_weights_path):
            size = os.path.getsize(lora_weights_path)
            print(f"âœ… LoRAæƒé‡æ–‡ä»¶å¤§å°: {size / 1024 / 1024:.2f} MB")

            # å°è¯•åŠ è½½æƒé‡æ£€æŸ¥
            try:
                weights = torch.load(lora_weights_path, map_location="cpu")
                print(f"âœ… LoRAæƒé‡å±‚æ•°: {len(weights)}")

                # æ£€æŸ¥æƒé‡ç»Ÿè®¡
                all_weights = torch.cat([w.flatten() for w in weights.values()])
                print(f"ğŸ“Š æƒé‡ç»Ÿè®¡:")
                print(f"  å‡å€¼: {all_weights.mean().item():.6f}")
                print(f"  æ ‡å‡†å·®: {all_weights.std().item():.6f}")
                print(f"  ç»å¯¹å€¼å‡å€¼: {all_weights.abs().mean().item():.6f}")

                if all_weights.abs().mean().item() < 1e-6:
                    print("âš ï¸ æƒé‡è¿‡å°ï¼Œå¯èƒ½è®­ç»ƒæœªç”Ÿæ•ˆ")
                    return False
                else:
                    print("âœ… æƒé‡æ­£å¸¸")
                    return True

            except Exception as e:
                print(f"âŒ åŠ è½½æƒé‡å¤±è´¥: {e}")
                return False
        else:
            print("âŒ æ‰¾ä¸åˆ°LoRAæƒé‡æ–‡ä»¶")
            return False
    else:
        print("ğŸ“ æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹ï¼ˆç¬¬ä¸€è½®è®­ç»ƒï¼‰")
        return True


def analyze_loss_pattern(first_epoch_loss):
    """åˆ†ælossæ¨¡å¼"""
    print(f"\nğŸ“ˆ åˆ†æç¬¬ä¸€è½®å¹³å‡Loss: {first_epoch_loss:.6f}")

    # åˆ¤æ–­lossæ˜¯å¦æ­£å¸¸
    if first_epoch_loss < 0.01:
        print("âŒ Lossè¿‡ä½ï¼Œå¯èƒ½è®­ç»ƒæ•°æ®æœ‰é—®é¢˜")
        return False
    elif first_epoch_loss > 0.5:
        print("âŒ Lossè¿‡é«˜ï¼Œå¯èƒ½å­¦ä¹ ç‡æœ‰é—®é¢˜æˆ–æ•°æ®å¼‚å¸¸")
        return False
    elif 0.08 <= first_epoch_loss <= 0.25:
        print("âœ… Lossåœ¨æ­£å¸¸èŒƒå›´å†…")
        return True
    else:
        print("âš ï¸ Lossç•¥å¼‚å¸¸ï¼Œä½†å¯èƒ½å¯ä»¥æ¥å—")
        return True


def generate_diagnostic_report(first_epoch_loss=None):
    """ç”Ÿæˆå®Œæ•´çš„è¯Šæ–­æŠ¥å‘Š"""
    print("ğŸ¥ å¼€å§‹å…¨é¢è¯Šæ–­...")
    print("=" * 60)

    results = {
        "cache_quality": check_cache_quality(),
        "data_integrity": check_data_integrity(),
        "training_progress": check_training_progress()
    }

    if first_epoch_loss is not None:
        results["loss_analysis"] = analyze_loss_pattern(first_epoch_loss)

    print("\n" + "=" * 60)
    print("ğŸ“‹ è¯Šæ–­æŠ¥å‘Šæ€»ç»“:")
    print("=" * 60)

    all_good = True
    for check, result in results.items():
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{check.replace('_', ' ').title()}: {status}")
        if not result:
            all_good = False

    print("=" * 60)
    if all_good:
        print("ğŸ‰ æ•´ä½“è¯„ä¼°: è®­ç»ƒçŠ¶æ€è‰¯å¥½ï¼Œå¯ä»¥ç»§ç»­ï¼")
        print("âœ… å»ºè®®: ç»§ç»­å½“å‰è®­ç»ƒï¼Œæ— éœ€é‡æ–°ç¼“å­˜")
        return "continue"
    else:
        print("âš ï¸ æ•´ä½“è¯„ä¼°: å‘ç°é—®é¢˜ï¼Œå»ºè®®ä¿®å¤")
        print("ğŸ”§ å»ºè®®: è€ƒè™‘ç¦ç”¨ç¼“å­˜é‡æ–°è®­ç»ƒ")
        return "restart"


# ä½¿ç”¨æ–¹æ³•
if __name__ == "__main__":
    print("ğŸ” LoRAè®­ç»ƒè¯Šæ–­å·¥å…·")
    print("è¯·åœ¨ç¬¬ä¸€è½®è®­ç»ƒå®Œæˆåè¿è¡Œæ­¤è„šæœ¬")
    print("\nå¦‚æœè¦åŒ…å«lossåˆ†æï¼Œè¯·è¿™æ ·è°ƒç”¨:")
    print("result = generate_diagnostic_report(first_epoch_loss=ä½ çš„å¹³å‡loss)")
    print("\nå¦‚æœåªæƒ³æ£€æŸ¥ç¼“å­˜å’Œæ•°æ®:")
    print("result = generate_diagnostic_report()")