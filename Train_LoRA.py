import json
from PIL import Image
import torch
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import os
from transformers import CLIPTokenizer, CLIPTextModel
from peft import LoraConfig
from diffusers import (
    AutoencoderKL,
    StableDiffusionControlNetPipeline,
    ControlNetModel,
    UniPCMultistepScheduler,
    UNet2DConditionModel,
)
from diffusers.models.attention_processor import LoRAAttnProcessor
from diffusers.training_utils import cast_training_params

# === é…ç½®å‚æ•° ===
DATA_DIR = "/content/drive/MyDrive/VisDrone2019-YOLO/VisDrone2019-YOLO-train/"
PROMPT_FILE = "prompt.jsonl"
OUTPUT_DIR = "/content/drive/MyDrive/VisDrone2019-YOLO/trained_lora_controlnet/"
os.makedirs(OUTPUT_DIR, exist_ok=True)
CHECKPOINT_DIR = "/content/drive/MyDrive/VisDrone2019-YOLO/checkpoints"
os.makedirs(CHECKPOINT_DIR, exist_ok=True)

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
BATCH_SIZE = 4
EPOCHS = 50
LR = 5e-5
MAX_TOKEN_LENGTH = 77
IMAGE_SIZE = 512

# === åŠ è½½ ControlNet å’Œ Pipeline ===
controlnet = ControlNetModel.from_pretrained(
    "lllyasviel/control_v11p_sd15_scribble", torch_dtype=torch.float32
)
pipe = StableDiffusionControlNetPipeline.from_pretrained(
    "stable-diffusion-v1-5/stable-diffusion-v1-5",
    controlnet=controlnet,
    torch_dtype=torch.float32,
)
pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
pipe = pipe.to(DEVICE)

# === æ³¨å…¥ LoRA æ³¨æ„åŠ›å¤„ç†å™¨ ===æ–°
unet_lora_config = LoraConfig(
    r=4,
    lora_alpha=4,
    init_lora_weights="gaussian",
    target_modules=["to_q", "to_k", "to_v", "to_out.0"],  # å®˜æ–¹æ¨èç»„åˆ
)
controlnet_lora_config = LoraConfig(
    r=4,
    lora_alpha=4,
    init_lora_weights="gaussian",
    target_modules=["to_q", "to_k", "to_v", "to_out.0"],
)

pipe.unet.add_adapter(unet_lora_config)
pipe.controlnet.add_adapter(controlnet_lora_config)     # æ–°

# === ç¡®ä¿ LoRA å‚æ•°ç”¨ float32 ç²¾åº¦è®­ç»ƒï¼ˆé˜²æ­¢æ··åˆç²¾åº¦å¼•å‘ä¸ç¨³å®šï¼‰ ===
cast_training_params(pipe.unet, dtype=torch.float32)
cast_training_params(pipe.controlnet, dtype=torch.float32)

# å†»ç»“æ‰€æœ‰å‚æ•° æ–°
pipe.unet.requires_grad_(False)
pipe.controlnet.requires_grad_(False)
pipe.text_encoder.requires_grad_(True)
pipe.text_encoder.train()

# === æ”¶é›†å‚æ•°å¹¶åˆ›å»ºä¼˜åŒ–å™¨ ===
trainable_params = []
for proc in pipe.unet.attn_processors.values():   # æ–°
    for p in proc.parameters():
        if p.requires_grad:
            trainable_params.append(p)
for proc in pipe.controlnet.attn_processors.values():
    for p in proc.parameters():
        if p.requires_grad:
            trainable_params.append(p)
trainable_params += list(pipe.text_encoder.parameters())
# å‚æ•°æ£€æŸ¥
print("âœ… å‚æ•°æ£€æŸ¥ï¼š")
for name, param in pipe.unet.named_parameters():
    if param.requires_grad:
        print(f"[UNet] è®­ç»ƒå‚æ•°: {name} - {param.shape}")
for name, param in pipe.controlnet.named_parameters():
    if param.requires_grad:
        print(f"[ControlNet] è®­ç»ƒå‚æ•°: {name} - {param.shape}")
for name, param in pipe.text_encoder.named_parameters():
    if param.requires_grad:
        print(f"[TextEncoder] è®­ç»ƒå‚æ•°: {name} - {param.shape}")
optimizer = torch.optim.AdamW(trainable_params, lr=LR)
trainable_count = sum(p.numel() for p in trainable_params if p.requires_grad)
print(f"ğŸ§® Optimizer ä¸­å¯è®­ç»ƒå‚æ•°æ€»æ•°: {trainable_count}")    # æ–°

pipe.unet.train()
pipe.controlnet.train()

# === åŠ è½½ Tokenizer ===
tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-large-patch14")

# === è‡ªå®šä¹‰æ•°æ®é›† ===
class VisDroneControlNetDataset(Dataset):
    def __init__(self, root_dir, prompt_file, tokenizer, max_length=MAX_TOKEN_LENGTH):
        self.root_dir = root_dir
        self.tokenizer = tokenizer
        self.max_length = max_length
        with open(os.path.join(root_dir, prompt_file), "r") as f:
            self.entries = [json.loads(line) for line in f]

    def __len__(self):
        return len(self.entries)

    def __getitem__(self, idx):
        item = self.entries[idx]
        image_path = os.path.join(self.root_dir, item["image_path"])
        layout_path = os.path.join(self.root_dir, item["layout_path"])
        prompt = item["prompt"]

        image = Image.open(image_path).convert("RGB").resize((IMAGE_SIZE, IMAGE_SIZE))
        layout = Image.open(layout_path).convert("RGB").resize((IMAGE_SIZE, IMAGE_SIZE))

        # ä½¿ç”¨pipeçš„feature_extractorå¤„ç†å›¾åƒå’Œå¸ƒå±€
        image_tensor = pipe.feature_extractor(images=image, return_tensors="pt").pixel_values[0]
        layout_tensor = pipe.feature_extractor(images=layout, return_tensors="pt").pixel_values[0]

        tokenized = self.tokenizer(
            prompt,
            padding="max_length",
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt",
        )

        return {
            "image": image_tensor,
            "layout": layout_tensor,
            "input_ids": tokenized.input_ids[0],
            "attention_mask": tokenized.attention_mask[0],
        }

dataset = VisDroneControlNetDataset(DATA_DIR, PROMPT_FILE, tokenizer)
dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True, prefetch_factor=2)

# === å°è¯•åŠ è½½æ–­ç‚¹ ===
start_epoch =1
for epoch in range(EPOCHS, 0, -1):
    unet_path = os.path.join(CHECKPOINT_DIR, f"unet_epoch_{epoch}")
    if os.path.exists(unet_path):
        print(f"æ¢å¤ epoch {epoch} çš„æ£€æŸ¥ç‚¹...")
        pipe.unet = pipe.unet.from_pretrained(unet_path).to(DEVICE)
        pipe.controlnet = pipe.controlnet.from_pretrained(os.path.join(CHECKPOINT_DIR, f"controlnet_epoch_{epoch}")).to(DEVICE)
        pipe.text_encoder = pipe.text_encoder.from_pretrained(os.path.join(CHECKPOINT_DIR, f"text_encoder_epoch_{epoch}")).to(DEVICE)
        optimizer.load_state_dict(torch.load(os.path.join(CHECKPOINT_DIR, f"optimizer_epoch_{epoch}.pt"), map_location=DEVICE))
        start_epoch = epoch + 1
        break

# === è®­ç»ƒå¾ªç¯ ===
for epoch in range(start_epoch, EPOCHS+1):
    pipe.unet.train()
    pipe.controlnet.train()
    pipe.text_encoder.train()

    loop = tqdm(dataloader, desc=f"Epoch {epoch}/{EPOCHS}")
    total_loss = 0
    step_count = 0
    for batch in loop:
        optimizer.zero_grad()

        # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
        image = batch["image"].to(DEVICE, dtype=torch.float32)
        layout = batch["layout"].to(DEVICE, dtype=torch.float32)
        input_ids = batch["input_ids"].to(DEVICE)
        attention_mask = batch["attention_mask"].to(DEVICE)

        # ç¼–ç æ–‡æœ¬
        encoder_hidden_states = pipe.text_encoder(input_ids=input_ids, attention_mask=attention_mask)[0].to(dtype=torch.float32).to(DEVICE)

        # ç¼–ç å›¾åƒè‡³latent
        latents = pipe.vae.encode(image).latent_dist.sample().to(DEVICE)
        latents = latents * pipe.vae.config.scaling_factor
        latents = latents.to(dtype=torch.float32)

        # é‡‡æ ·éšæœºæ—¶é—´æ­¥
        timesteps = torch.randint(0, pipe.scheduler.config.num_train_timesteps, (latents.shape[0],), device=DEVICE).long()

        # æ·»åŠ å™ªå£°
        noise = torch.randn_like(latents).to(DEVICE)
        noisy_latents = pipe.scheduler.add_noise(latents, noise, timesteps)

        # ControlNetå‰å‘
        controlnet_out = pipe.controlnet(
            sample=noisy_latents,
            timestep=timesteps,
            encoder_hidden_states=encoder_hidden_states,
            controlnet_cond=layout,
            return_dict=True,
        )

        # UNetå‰å‘ï¼ŒèåˆControlNetè¾“å‡º
        unet_out = pipe.unet(
            sample=noisy_latents,
            timestep=timesteps,
            encoder_hidden_states=encoder_hidden_states,
            down_block_additional_residuals=controlnet_out.down_block_res_samples,
            mid_block_additional_residual=controlnet_out.mid_block_res_sample,
            return_dict=True,
        )

        # é¢„æµ‹å™ªå£°
        noise_pred = unet_out.sample

        # è®¡ç®—æŸå¤±
        loss = torch.nn.functional.mse_loss(noise_pred, noise)
        loss.backward()
        for name, param in pipe.unet.named_parameters():  # æ–°
            if param.requires_grad and param.grad is not None:
                print(f"ğŸ“ˆ æ¢¯åº¦æ£€æŸ¥: {name} çš„æ¢¯åº¦å‡å€¼: {param.grad.abs().mean().item():.6f}")
                break   # æ–°
        torch.nn.utils.clip_grad_norm_(list(pipe.unet.parameters()) +
                                       list(pipe.controlnet.parameters()) +
                                       list(pipe.text_encoder.parameters()), max_norm=1.0)
        optimizer.step()
        total_loss += loss.item()
        step_count += 1
        loop.set_postfix(loss=loss.item())

    avg_loss = total_loss / step_count if step_count > 0 else 0
    print(f"å¹³å‡ Lossï¼ˆEpoch {epoch}ï¼‰: {avg_loss:.6f}")

    if epoch == start_epoch:     # æ–°
        initial_param_snapshot = pipe.unet.attn_processors[
            "mid_block.attn1.processor"].to_q_lora.lora_A.weight.detach().clone()
    elif epoch > start_epoch:
        current_param = pipe.unet.attn_processors["mid_block.attn1.processor"].to_q_lora.lora_A.weight
        delta = (current_param - initial_param_snapshot).abs().mean().item()
        print(f"ğŸ§ª å‚æ•°å˜åŒ–å‡å€¼ (mid_block to_q): {delta:.6f}")    # æ–°

    if epoch == EPOCHS:
        pipe.unet.save_pretrained(os.path.join(OUTPUT_DIR, "unet"))
        pipe.controlnet.save_pretrained(os.path.join(OUTPUT_DIR, "controlnet"))
        pipe.text_encoder.save_pretrained(os.path.join(OUTPUT_DIR, "text_encoder"))
        torch.save(optimizer.state_dict(), os.path.join(OUTPUT_DIR, "optimizer.pt"))
        print("è®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆæ¨¡å‹å·²ä¿å­˜ã€‚")
    else:
        pipe.unet.save_pretrained(os.path.join(CHECKPOINT_DIR, f"unet_epoch_{epoch}"))
        pipe.controlnet.save_pretrained(os.path.join(CHECKPOINT_DIR, f"controlnet_epoch_{epoch}"))
        pipe.text_encoder.save_pretrained(os.path.join(CHECKPOINT_DIR, f"text_encoder_epoch_{epoch}"))
        torch.save(optimizer.state_dict(), os.path.join(CHECKPOINT_DIR, f"optimizer_epoch_{epoch}.pt"))
        print(f"ä¿å­˜ epoch {epoch} çš„æƒé‡å®Œæˆã€‚")
