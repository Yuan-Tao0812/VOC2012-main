import os
import json
import torch
from accelerate import Accelerator
from PIL import Image
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
from torchvision import transforms

from diffusers import (
    UNet2DConditionModel,
    AutoencoderKL,
    DDPMScheduler,
    StableDiffusionPipeline,
)
from diffusers.training_utils import cast_training_params
from diffusers.optimization import get_scheduler
from diffusers.utils import convert_state_dict_to_diffusers
from transformers import CLIPTextModel, CLIPTokenizer
from peft import LoraConfig, get_peft_model_state_dict

DATA_DIR = "/content/drive/MyDrive/VisDrone2019-YOLO/VisDrone2019-YOLO-renamed/"
PROMPT_FILE = "prompt.jsonl"
OUTPUT_DIR = "/content/drive/MyDrive/VisDrone2019-YOLO/trained_lora_controlnet/"
CHECKPOINT_DIR = "/content/drive/MyDrive/VisDrone2019-YOLO/checkpoints"
PRETRAINED_MODEL_PATH = "stable-diffusion-v1-5/stable-diffusion-v1-5"  # ä¿®æ­£çš„æ¨¡å‹è·¯å¾„

os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(CHECKPOINT_DIR, exist_ok=True)

# ä¼˜åŒ–è®­ç»ƒå‚æ•°ä»¥æé«˜é€Ÿåº¦
BATCH_SIZE = 8  # A100å¯ä»¥æ”¯æŒæ›´å¤§batch size
EPOCHS = 12  # å‡å°‘epochsï¼Œå…ˆçœ‹æ•ˆæœ
LR = 1e-4  # ä¿æŒå®˜æ–¹æ¨èå­¦ä¹ ç‡
GRADIENT_ACCUMULATION_STEPS = 1
SCALE_LR = True
MAX_TOKEN_LENGTH = 77
IMAGE_SIZE = 512
CACHE_LATENTS = True  # ç¼“å­˜latentsä»¥å¤§å¹…æé€Ÿ
epoch_losses = []
weight_dtype = torch.float16 if torch.cuda.is_available() else torch.float32

# åˆå§‹åŒ–Accelerator - ä¼˜åŒ–è®¾ç½®
accelerator = Accelerator(
    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
    mixed_precision="fp16" if torch.cuda.is_available() else "no",
    dataloader_config={
        "num_workers": 4,  # å¢åŠ æ•°æ®åŠ è½½å¹¶è¡Œåº¦
        "pin_memory": True,
        "persistent_workers": True,  # ä¿æŒworkerè¿›ç¨‹ï¼Œå‡å°‘é‡å¯å¼€é”€
    }
)

print(f"ä½¿ç”¨è®¾å¤‡: {accelerator.device}")
print(f"æ··åˆç²¾åº¦: {accelerator.mixed_precision}")

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
noise_scheduler = DDPMScheduler.from_pretrained(PRETRAINED_MODEL_PATH, subfolder="scheduler")
tokenizer = CLIPTokenizer.from_pretrained(PRETRAINED_MODEL_PATH, subfolder="tokenizer")
text_encoder = CLIPTextModel.from_pretrained(PRETRAINED_MODEL_PATH, subfolder="text_encoder")
vae = AutoencoderKL.from_pretrained(PRETRAINED_MODEL_PATH, subfolder="vae")
unet = UNet2DConditionModel.from_pretrained(PRETRAINED_MODEL_PATH, subfolder="unet")

# å†»ç»“é¢„è®­ç»ƒå‚æ•°
unet.requires_grad_(False)
vae.requires_grad_(False)
text_encoder.requires_grad_(False)

# LoRAé…ç½®
unet_lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    init_lora_weights="gaussian",
    target_modules=["to_k", "to_q", "to_v", "to_out.0"],
)

# ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡
text_encoder = text_encoder.to(accelerator.device, dtype=weight_dtype)
vae = vae.to(accelerator.device, dtype=weight_dtype)
unet = unet.to(accelerator.device, dtype=weight_dtype)

# æ·»åŠ LoRAé€‚é…å™¨ï¼ˆå®˜æ–¹æ–¹å¼ï¼‰
unet.add_adapter(unet_lora_config)

# è®¾ç½®è®­ç»ƒå‚æ•°
cast_training_params(unet, dtype=torch.float32)

# è·å–LoRAå¯è®­ç»ƒå‚æ•°
lora_layers = list(filter(lambda p: p.requires_grad, unet.parameters()))
print(f"å¯è®­ç»ƒå‚æ•°æ•°é‡ï¼ˆLoRAï¼‰ï¼š{len(lora_layers)}")
print(f"å¯è®­ç»ƒå‚æ•°æ€»é‡ï¼š{sum(p.numel() for p in lora_layers)}")

# éªŒè¯LoRAå‚æ•°
print("\néƒ¨åˆ†LoRAå‚æ•°ç¤ºä¾‹ï¼š")
count = 0
for name, param in unet.named_parameters():
    if param.requires_grad and count < 5:
        print(f"  {name}: shape={param.shape}, requires_grad={param.requires_grad}")
        count += 1

# å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
if hasattr(unet, "enable_gradient_checkpointing"):
    unet.enable_gradient_checkpointing()

# ä¼˜åŒ–å™¨
optimizer = torch.optim.AdamW(
    lora_layers,
    lr=LR,
    betas=(0.9, 0.999),
    weight_decay=1e-2,
    eps=1e-8,
)

# å­¦ä¹ ç‡ç¼©æ”¾ï¼ˆæŒ‰å®˜æ–¹é€»è¾‘ï¼‰
if SCALE_LR:
    scaled_lr = LR * GRADIENT_ACCUMULATION_STEPS * BATCH_SIZE * accelerator.num_processes
    print(f"åŸå§‹å­¦ä¹ ç‡: {LR}")
    print(f"ç¼©æ”¾åå­¦ä¹ ç‡: {scaled_lr}")
    # é‡æ–°åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        lora_layers,
        lr=scaled_lr,
        betas=(0.9, 0.999),
        weight_decay=1e-2,
        eps=1e-8,
    )

# æ•°æ®é¢„å¤„ç†
transform = transforms.Compose([
    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.5] * 3, [0.5] * 3),
])


class VisDroneControlNetDataset(torch.utils.data.Dataset):
    def __init__(self, root_dir, prompt_file, tokenizer, vae=None, max_length=MAX_TOKEN_LENGTH, transform=None):
        self.root_dir = root_dir
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.transform = transform
        self.vae = vae
        self.cache_latents = CACHE_LATENTS and vae is not None

        prompt_path = os.path.join(root_dir, prompt_file)
        if not os.path.exists(prompt_path):
            raise FileNotFoundError(f"Prompt file not found: {prompt_path}")

        with open(prompt_path, "r") as f:
            self.entries = [json.loads(line) for line in f]

        print(f"åŠ è½½äº† {len(self.entries)} ä¸ªè®­ç»ƒæ ·æœ¬")

        # é¢„ç¼“å­˜latentsä»¥æé«˜è®­ç»ƒé€Ÿåº¦
        if self.cache_latents:
            print("é¢„ç¼“å­˜latentsä¸­...")
            self._cache_latents()

    def _cache_latents(self):
        """é¢„ç¼“å­˜æ‰€æœ‰å›¾ç‰‡çš„latents"""
        self.cached_latents = []
        cache_dir = os.path.join(self.root_dir, "cached_latents")
        os.makedirs(cache_dir, exist_ok=True)

        for idx, item in enumerate(tqdm(self.entries, desc="ç¼“å­˜latents")):
            cache_file = os.path.join(cache_dir, f"latent_{idx}.pt")

            if os.path.exists(cache_file):
                # åŠ è½½å·²ç¼“å­˜çš„latent
                latent = torch.load(cache_file, map_location="cpu")
            else:
                # ç”Ÿæˆå¹¶ä¿å­˜latent
                image_path = os.path.join(self.root_dir, item["image"])
                image = Image.open(image_path).convert("RGB")

                if self.transform:
                    image = self.transform(image)

                # ç¼–ç ä¸ºlatent
                with torch.no_grad():
                    image_tensor = image.unsqueeze(0).to(self.vae.device, dtype=self.vae.dtype)
                    latent = self.vae.encode(image_tensor).latent_dist.sample()
                    latent = latent * self.vae.config.scaling_factor
                    latent = latent.squeeze(0).cpu()

                # ä¿å­˜ç¼“å­˜
                torch.save(latent, cache_file)

            self.cached_latents.append(latent)

    def __len__(self):
        return len(self.entries)

    def __getitem__(self, idx):
        item = self.entries[idx]

        if self.cache_latents:
            # ä½¿ç”¨ç¼“å­˜çš„latents
            latent = self.cached_latents[idx]
            image = None  # ä¸éœ€è¦åŸå§‹å›¾ç‰‡
        else:
            # åŸå§‹æ–¹å¼
            image_path = os.path.join(self.root_dir, item["image"])

            if not os.path.exists(image_path):
                raise FileNotFoundError(f"Image not found: {image_path}")

            image = Image.open(image_path).convert("RGB")
            if self.transform:
                image = self.transform(image)
            latent = None

        tokenized = self.tokenizer(
            item["prompt"],
            padding="max_length",
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt",
        )

        result = {
            "input_ids": tokenized.input_ids[0],
            "attention_mask": tokenized.attention_mask[0],
        }

        if self.cache_latents:
            result["latents"] = latent
        else:
            result["image"] = image

        return result


# åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨ - æ”¯æŒlatentç¼“å­˜
dataset = VisDroneControlNetDataset(
    DATA_DIR,
    PROMPT_FILE,
    tokenizer,
    vae=vae if CACHE_LATENTS else None,  # ä¼ å…¥VAEç”¨äºç¼“å­˜
    transform=transform
)


def collate_fn(examples):
    input_ids = torch.stack([ex["input_ids"] for ex in examples])
    attention_mask = torch.stack([ex["attention_mask"] for ex in examples])

    result = {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
    }

    if CACHE_LATENTS:
        latents = torch.stack([ex["latents"] for ex in examples])
        result["latents"] = latents
    else:
        pixel_values = torch.stack([ex["image"] for ex in examples])
        result["pixel_values"] = pixel_values

    return result


dataloader = DataLoader(
    dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    collate_fn=collate_fn,
    num_workers=4,  # å¢åŠ workeræ•°é‡
    pin_memory=True,
    persistent_workers=True,  # ä¿æŒworkerè¿›ç¨‹
    prefetch_factor=2,  # é¢„å–æ•°æ®
)

# å­¦ä¹ ç‡è°ƒåº¦å™¨
num_update_steps_per_epoch = len(dataloader) // GRADIENT_ACCUMULATION_STEPS
max_train_steps = EPOCHS * num_update_steps_per_epoch

print(f"æ¯epochæ›´æ–°æ­¥æ•°: {num_update_steps_per_epoch}")
print(f"æ€»è®­ç»ƒæ­¥æ•°: {max_train_steps}")

lr_scheduler = get_scheduler(
    "cosine",
    optimizer=optimizer,
    num_warmup_steps=500,
    num_training_steps=max_train_steps,
)


# å®˜æ–¹ä¿å­˜æ–¹å¼çš„è¾…åŠ©å‡½æ•°
def unwrap_model(model):
    model = accelerator.unwrap_model(model)
    return model


def save_lora_checkpoint(epoch, unet, optimizer, lr_scheduler, loss):
    """æŒ‰ç…§å®˜æ–¹æ–¹å¼ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆåŒ…å«å®Œæ•´è®­ç»ƒçŠ¶æ€ï¼‰"""
    # 1. ä½¿ç”¨acceleratorä¿å­˜å®Œæ•´æ£€æŸ¥ç‚¹ï¼ˆç”¨äºæ–­ç‚¹ç»­è®­ï¼‰
    checkpoint_path = os.path.join(CHECKPOINT_DIR, f"checkpoint-{epoch * len(dataloader)}")
    accelerator.save_state(checkpoint_path)

    # 2. é¢å¤–ä¿å­˜çº¯LoRAæƒé‡ï¼ˆç”¨äºæ¨ç†ï¼‰
    lora_path = os.path.join(CHECKPOINT_DIR, f"lora_epoch_{epoch}")
    os.makedirs(lora_path, exist_ok=True)

    unwrapped_unet = unwrap_model(unet)
    unet_lora_state_dict = convert_state_dict_to_diffusers(get_peft_model_state_dict(unwrapped_unet))

    StableDiffusionPipeline.save_lora_weights(
        save_directory=lora_path,
        unet_lora_layers=unet_lora_state_dict,
        safe_serialization=False,  # åŒ¹é…å®˜æ–¹ .bin æ ¼å¼
    )

    print(f"å®Œæ•´æ£€æŸ¥ç‚¹ä¿å­˜åˆ°: {checkpoint_path}")
    print(f"LoRAæƒé‡ä¿å­˜åˆ°: {lora_path}")


def load_lora_checkpoint():
    """åŠ è½½æœ€æ–°çš„æ£€æŸ¥ç‚¹ï¼ˆAcceleratoræ ¼å¼ï¼‰"""
    start_epoch = 1

    # æŸ¥æ‰¾acceleratoræ ¼å¼çš„æ£€æŸ¥ç‚¹
    checkpoints = []
    for item in os.listdir(CHECKPOINT_DIR):
        if item.startswith("checkpoint-"):
            step_num = int(item.split("-")[1])
            checkpoints.append((step_num, item))

    if checkpoints:
        # æŒ‰æ­¥æ•°æ’åºï¼Œå–æœ€æ–°çš„
        checkpoints.sort(reverse=True)
        latest_checkpoint = checkpoints[0][1]
        checkpoint_path = os.path.join(CHECKPOINT_DIR, latest_checkpoint)

        print(f"å‘ç°æœ€æ–°æ£€æŸ¥ç‚¹: {latest_checkpoint}")

        try:
            # ä½¿ç”¨acceleratoråŠ è½½çŠ¶æ€
            accelerator.load_state(checkpoint_path)

            # è®¡ç®—å¯¹åº”çš„epoch
            step_num = checkpoints[0][0]
            start_epoch = (step_num // len(dataloader)) + 1

            print(f"æˆåŠŸåŠ è½½æ£€æŸ¥ç‚¹ï¼Œå°†ä» epoch {start_epoch} å¼€å§‹è®­ç»ƒ")

        except Exception as e:
            print(f"åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            start_epoch = 1

    return start_epoch


# å°è¯•åŠ è½½æ£€æŸ¥ç‚¹
start_epoch = load_lora_checkpoint()

# å‡†å¤‡è®­ç»ƒ
unet, optimizer, dataloader, lr_scheduler = accelerator.prepare(
    unet, optimizer, dataloader, lr_scheduler
)


# ç›‘æ§LoRAå‚æ•°å˜åŒ–çš„å‡½æ•°
def get_lora_param_stats(model):
    """è·å–LoRAå‚æ•°çš„ç»Ÿè®¡ä¿¡æ¯"""
    lora_params = []
    for name, param in model.named_parameters():
        if param.requires_grad and any(lora_key in name for lora_key in ["lora_A", "lora_B"]):
            lora_params.append(param.detach().cpu())

    if lora_params:
        all_params = torch.cat([p.flatten() for p in lora_params])
        return {
            "mean": all_params.mean().item(),
            "std": all_params.std().item(),
            "min": all_params.min().item(),
            "max": all_params.max().item()
        }
    return None


print(f"å¼€å§‹è®­ç»ƒï¼Œä» epoch {start_epoch} åˆ° {EPOCHS}")
print(f"æ€»è®­ç»ƒæ­¥æ•°: {max_train_steps}")
print(f"æ¯epochæ­¥æ•°: {num_update_steps_per_epoch}")
print(f"ä½¿ç”¨æ‰¹æ¬¡å¤§å°: {BATCH_SIZE} (å®é™…å­¦ä¹ ç‡å·²ç¼©æ”¾)")
if CACHE_LATENTS:
    print("âœ… å¯ç”¨latentç¼“å­˜ï¼Œå¤§å¹…æå‡è®­ç»ƒé€Ÿåº¦")
print(f"é¢„è®¡æ¯epochæ—¶é—´: çº¦ 45-60åˆ†é’Ÿ (6471å¼ å›¾)")
print(f"é¢„è®¡æ€»è®­ç»ƒæ—¶é—´: çº¦ {EPOCHS * 0.8:.1f}-{EPOCHS * 1.0:.1f} å°æ—¶")

# è®­ç»ƒå¾ªç¯
for epoch in range(start_epoch, EPOCHS + 1):
    unet.train()
    total_loss = 0.0

    # è®­ç»ƒå‰LoRAå‚æ•°ç»Ÿè®¡
    lora_stats_before = get_lora_param_stats(unet)
    if lora_stats_before:
        print(f"\nEpoch {epoch} å¼€å§‹ - LoRAå‚æ•°ç»Ÿè®¡:")
        print(f"  å‡å€¼: {lora_stats_before['mean']:.6f}")
        print(f"  æ ‡å‡†å·®: {lora_stats_before['std']:.6f}")

    progress_bar = tqdm(dataloader, desc=f"Epoch {epoch}/{EPOCHS}")

    for step, batch in enumerate(progress_bar):
        with accelerator.accumulate(unet):
            # è·å–batchæ•°æ®
            input_ids = batch["input_ids"].to(accelerator.device)
            attention_mask = batch["attention_mask"].to(accelerator.device)

            # ç¼–ç æ–‡æœ¬
            with torch.no_grad():
                encoder_hidden_states = text_encoder(
                    input_ids=input_ids,
                    attention_mask=attention_mask
                )[0]

            # è·å–latents - ä½¿ç”¨ç¼“å­˜æˆ–å®æ—¶ç¼–ç 
            if CACHE_LATENTS:
                latents = batch["latents"].to(accelerator.device, dtype=weight_dtype)
            else:
                pixel_values = batch["pixel_values"].to(accelerator.device, dtype=weight_dtype)
                with torch.no_grad():
                    latents = vae.encode(pixel_values).latent_dist.sample()
                    latents = latents * vae.config.scaling_factor

            # æ·»åŠ å™ªå£°
            noise = torch.randn_like(latents, device=accelerator.device, dtype=weight_dtype)
            timesteps = torch.randint(
                0, noise_scheduler.config.num_train_timesteps,
                (latents.shape[0],),
                device=accelerator.device
            ).long()

            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)

            # UNetå‰å‘ä¼ æ’­
            model_pred = unet(
                sample=noisy_latents,
                timestep=timesteps,
                encoder_hidden_states=encoder_hidden_states,
                return_dict=False
            )[0]

            # è®¡ç®—æŸå¤±
            if noise_scheduler.config.prediction_type == "epsilon":
                target = noise
            elif noise_scheduler.config.prediction_type == "v_prediction":
                target = noise_scheduler.get_velocity(latents, noise, timesteps)
            else:
                raise ValueError(f"Unknown prediction type {noise_scheduler.config.prediction_type}")

            loss = torch.nn.functional.mse_loss(model_pred.float(), target.float(), reduction="mean")

            # åå‘ä¼ æ’­
            accelerator.backward(loss)

            if accelerator.sync_gradients:
                accelerator.clip_grad_norm_(lora_layers, 1.0)

            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()

            # æ›´æ–°è¿›åº¦æ¡
            current_loss = loss.detach().item()
            total_loss += current_loss
            progress_bar.set_postfix({
                "loss": f"{current_loss:.4f}",
                "lr": f"{lr_scheduler.get_last_lr()[0]:.2e}",
                "step": f"{step}/{len(dataloader)}"
            })

    # epochç»“æŸç»Ÿè®¡
    avg_loss = total_loss / len(dataloader)
    epoch_losses.append(avg_loss)

    # è®­ç»ƒåLoRAå‚æ•°ç»Ÿè®¡
    lora_stats_after = get_lora_param_stats(unet)
    if lora_stats_after and lora_stats_before:
        mean_change = abs(lora_stats_after['mean'] - lora_stats_before['mean'])
        print(f"\nEpoch {epoch} ç»“æŸ:")
        print(f"  å¹³å‡Loss: {avg_loss:.6f}")
        print(f"  LoRAå‚æ•°å‡å€¼å˜åŒ–: {mean_change:.8f}")

        if mean_change < 1e-8:
            print(f"  âš ï¸  è­¦å‘Š: LoRAå‚æ•°å˜åŒ–æå°ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´å­¦ä¹ ç‡æˆ–æ£€æŸ¥æ¢¯åº¦")

    # ä¿å­˜æ£€æŸ¥ç‚¹
    if epoch % 5 == 0 or epoch == EPOCHS:
        save_lora_checkpoint(epoch, unet, optimizer, lr_scheduler, avg_loss)

print("è®­ç»ƒå®Œæˆ!")

# æœ€ç»ˆä¿å­˜ï¼ˆå®˜æ–¹æ–¹å¼ï¼‰
print("ä¿å­˜æœ€ç»ˆLoRAæ¨¡å‹...")
final_output_path = OUTPUT_DIR

# è·å–LoRAæƒé‡ï¼ˆå®˜æ–¹æ–¹å¼ï¼‰
unwrapped_unet = unwrap_model(unet)
unet_lora_state_dict = convert_state_dict_to_diffusers(get_peft_model_state_dict(unwrapped_unet))

# ä¿å­˜LoRAæƒé‡ï¼ˆå®˜æ–¹æ–¹å¼ï¼‰- ä¿å­˜ä¸º .bin æ ¼å¼ä»¥åŒ¹é…å®˜æ–¹ç¤ºä¾‹
StableDiffusionPipeline.save_lora_weights(
    save_directory=final_output_path,
    unet_lora_layers=unet_lora_state_dict,
    safe_serialization=False,  # ä¿å­˜ä¸º pytorch_lora_weights.bin æ ¼å¼
)

# ä¿å­˜é…ç½®æ–‡ä»¶
config_info = {
    "lora_config": {
        "r": unet_lora_config.r,
        "lora_alpha": unet_lora_config.lora_alpha,
        "target_modules": unet_lora_config.target_modules,
        "init_lora_weights": unet_lora_config.init_lora_weights
    },
    "training_info": {
        "epochs": EPOCHS,
        "batch_size": BATCH_SIZE,
        "learning_rate": LR,
        "final_loss": epoch_losses[-1] if epoch_losses else None
    }
}

with open(os.path.join(final_output_path, "training_config.json"), "w") as f:
    json.dump(config_info, f, indent=2)

# ç»˜åˆ¶lossæ›²çº¿
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(range(1, len(epoch_losses) + 1), epoch_losses, marker='o', label="Average Training Loss", linewidth=2)
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training Loss Curve")
plt.legend()
plt.grid(True, alpha=0.3)

# Losså˜åŒ–ç‡
if len(epoch_losses) > 1:
    plt.subplot(1, 2, 2)
    loss_changes = [epoch_losses[i] - epoch_losses[i - 1] for i in range(1, len(epoch_losses))]
    plt.plot(range(2, len(epoch_losses) + 1), loss_changes, marker='s', color='orange', label="Loss Change",
             linewidth=2)
    plt.xlabel("Epoch")
    plt.ylabel("Loss Change")
    plt.title("Loss Change Rate")
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.axhline(y=0, color='red', linestyle='--', alpha=0.7)

plt.tight_layout()
plt.savefig(os.path.join(OUTPUT_DIR, "training_curves.png"), dpi=150, bbox_inches='tight')
plt.show()

print(f"æœ€ç»ˆLoRAæ¨¡å‹ä¿å­˜åœ¨: {final_output_path}")
print(f"ä¸»è¦æ–‡ä»¶:")
print(f"  - pytorch_lora_weights.bin (æœ€ç»ˆLoRAæƒé‡ï¼Œç”¨äºæ¨ç†)")
print(f"  - training_config.json (è®­ç»ƒé…ç½®)")
print(f"  - training_curves.png (Lossæ›²çº¿)")
print(f"\næ£€æŸ¥ç‚¹æ–‡ä»¶ä¿å­˜åœ¨: {CHECKPOINT_DIR}")
print(f"  - checkpoint-xxxx/ (å®Œæ•´è®­ç»ƒçŠ¶æ€ï¼Œç”¨äºæ–­ç‚¹ç»­è®­)")
print(f"  - lora_epoch_x/ (LoRAæƒé‡å¤‡ä»½)")

print("\n" + "=" * 60)
print("ğŸ¨ å¼€å§‹æ¨ç†æµ‹è¯•...")
print("=" * 60)

# æ¸…ç†GPUå†…å­˜ï¼Œä¸ºæ¨ç†åšå‡†å¤‡
del unet, text_encoder, vae, optimizer, lr_scheduler
torch.cuda.empty_cache()

try:
    # åŠ è½½æ¨ç†pipeline
    print("æ­£åœ¨åŠ è½½æ¨ç†æ¨¡å‹...")
    inference_pipeline = StableDiffusionPipeline.from_pretrained(
        PRETRAINED_MODEL_PATH,
        torch_dtype=torch.float16,
        safety_checker=None,  # å…³é—­å®‰å…¨æ£€æŸ¥ä»¥èŠ‚çœå†…å­˜
        requires_safety_checker=False
    ).to("cuda")

    # åŠ è½½è®­ç»ƒå¥½çš„LoRAæƒé‡
    print("æ­£åœ¨åŠ è½½LoRAæƒé‡...")
    inference_pipeline.load_lora_weights(final_output_path)

    # æµ‹è¯•prompts - åŸºäºä½ çš„VisDroneæ•°æ®é›†é£æ ¼
    test_prompts = [
        "There are 5 pedestrians, 1 van, and 2 trucks in the image.",
        "There are 10 pedestrians, 3 cars, and 1 bus in the image.",
        "There are 2 pedestrians, 4 cars, and 2 motorcycles in the image.",
        "There are 15 pedestrians, 2 vans, 1 truck, and 3 cars in the image.",
        "There are 8 pedestrians, 1 van, and 5 cars in the image."
    ]

    # ç”Ÿæˆæµ‹è¯•å›¾ç‰‡
    inference_results_dir = os.path.join(OUTPUT_DIR, "inference_results")
    os.makedirs(inference_results_dir, exist_ok=True)

    print(f"æ­£åœ¨ç”Ÿæˆ {len(test_prompts)} å¼ æµ‹è¯•å›¾ç‰‡...")

    for i, prompt in enumerate(test_prompts):
        print(f"ç”Ÿæˆå›¾ç‰‡ {i + 1}/{len(test_prompts)}: {prompt}")

        # ç”Ÿæˆå›¾ç‰‡
        with torch.no_grad():
            image = inference_pipeline(
                prompt,
                num_inference_steps=25,  # é€‚ä¸­çš„æ­¥æ•°ï¼Œå¹³è¡¡è´¨é‡å’Œé€Ÿåº¦
                guidance_scale=7.5,
                generator=torch.manual_seed(42 + i)  # å›ºå®šéšæœºç§å­ä¾¿äºæ¯”è¾ƒ
            ).images[0]

        # ä¿å­˜å›¾ç‰‡
        image_path = os.path.join(inference_results_dir, f"test_image_{i + 1}.png")
        image.save(image_path)

        # ä¿å­˜promptä¿¡æ¯
        prompt_info = {
            "image": f"test_image_{i + 1}.png",
            "prompt": prompt,
            "inference_steps": 25,
            "guidance_scale": 7.5,
            "seed": 42 + i
        }

        info_path = os.path.join(inference_results_dir, f"test_image_{i + 1}_info.json")
        with open(info_path, "w") as f:
            json.dump(prompt_info, f, indent=2)

    # åˆ›å»ºç»“æœæ€»è§ˆå›¾
    print("åˆ›å»ºç»“æœæ€»è§ˆ...")
    from PIL import Image as PILImage
    import matplotlib.pyplot as plt

    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('LoRA Training Results - Generated Images', fontsize=16, fontweight='bold')

    for i, prompt in enumerate(test_prompts):
        row = i // 3
        col = i % 3

        image_path = os.path.join(inference_results_dir, f"test_image_{i + 1}.png")
        img = PILImage.open(image_path)

        axes[row, col].imshow(img)
        axes[row, col].set_title(f"Test {i + 1}", fontsize=12, fontweight='bold')
        axes[row, col].text(0.02, 0.98, prompt, transform=axes[row, col].transAxes,
                            fontsize=8, verticalalignment='top',
                            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
        axes[row, col].axis('off')

    # éšè—æœ€åä¸€ä¸ªç©ºçš„subplot
    axes[1, 2].axis('off')

    plt.tight_layout()
    overview_path = os.path.join(inference_results_dir, "results_overview.png")
    plt.savefig(overview_path, dpi=150, bbox_inches='tight')
    plt.close()

    print("\n" + "=" * 60)
    print("ğŸ‰ æ¨ç†æµ‹è¯•å®Œæˆï¼")
    print("=" * 60)
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {inference_results_dir}")
    print(f"ğŸ–¼ï¸  ç”Ÿæˆçš„å›¾ç‰‡:")
    for i in range(len(test_prompts)):
        print(f"   - test_image_{i + 1}.png")
    print(f"ğŸ“Š ç»“æœæ€»è§ˆ: results_overview.png")
    print(f"ğŸ“‹ æ¯å¼ å›¾ç‰‡éƒ½æœ‰å¯¹åº”çš„ _info.json æ–‡ä»¶è®°å½•ç”Ÿæˆå‚æ•°")

    print(f"\nğŸ” è®­ç»ƒæ•ˆæœåˆ†æ:")
    print(f"è¯·æ£€æŸ¥ç”Ÿæˆçš„å›¾ç‰‡æ˜¯å¦:")
    print(f"  âœ“ åŒ…å«promptä¸­æŒ‡å®šçš„ç‰©ä½“ç±»å‹å’Œæ•°é‡")
    print(f"  âœ“ å…·æœ‰èˆªæ‹/ä¿¯è§†è§’åº¦çš„é£æ ¼")
    print(f"  âœ“ ç”»é¢æ¸…æ™°ï¼Œç‰©ä½“å¯è¯†åˆ«")
    print(f"  âœ“ æ•´ä½“é£æ ¼ä¸VisDroneæ•°æ®é›†ç›¸ä¼¼")

    # æ¸…ç†æ¨ç†pipelineå†…å­˜
    del inference_pipeline
    torch.cuda.empty_cache()

except Exception as e:
    print(f"âŒ æ¨ç†æµ‹è¯•å¤±è´¥: {e}")
    print("è¯·æ£€æŸ¥:")
    print("  - GPUå†…å­˜æ˜¯å¦å……è¶³")
    print("  - LoRAæƒé‡æ–‡ä»¶æ˜¯å¦æ­£ç¡®ä¿å­˜")
    print("  - åŸºç¡€æ¨¡å‹æ˜¯å¦èƒ½æ­£å¸¸åŠ è½½")

    # ä¿å­˜é”™è¯¯ä¿¡æ¯ä¾›è°ƒè¯•
    error_info = {
        "error": str(e),
        "error_type": type(e).__name__,
        "suggestion": "æ£€æŸ¥GPUå†…å­˜å’Œæ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§"
    }

    error_path = os.path.join(OUTPUT_DIR, "inference_error.json")
    with open(error_path, "w") as f:
        json.dump(error_info, f, indent=2)

    print(f"é”™è¯¯ä¿¡æ¯å·²ä¿å­˜åˆ°: {error_path}")

print("\nğŸŒ™ è®­ç»ƒå’Œæµ‹è¯•å…¨éƒ¨å®Œæˆï¼å¯ä»¥å®‰å¿ƒç¡è§‰äº†ï½")
print(f"èµ·åºŠåæŸ¥çœ‹ç»“æœç›®å½•: {OUTPUT_DIR}")

print(f"\nç°åœ¨çš„æ–‡ä»¶ç»“æ„å’Œå®˜æ–¹ sayakpaul/sd-model-finetuned-lora-t4 å®Œå…¨ä¸€è‡´ï¼")
print(f"ä¿å­˜çš„æ˜¯ pytorch_lora_weights.bin æ ¼å¼ï¼Œä¸å®˜æ–¹ç¤ºä¾‹å®Œå…¨ç›¸åŒã€‚")