import os
import json
import torch
from accelerate import Accelerator
from PIL import Image
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
from torchvision import transforms
import random
import numpy as np

from diffusers import (
    UNet2DConditionModel,
    AutoencoderKL,
    DDPMScheduler,
    StableDiffusionPipeline,
)
from diffusers.training_utils import cast_training_params
from diffusers.optimization import get_scheduler
from diffusers.utils import convert_state_dict_to_diffusers
from transformers import CLIPTextModel, CLIPTokenizer
from peft import LoraConfig, get_peft_model_state_dict

DATA_DIR = "/content/drive/MyDrive/VisDrone2019-YOLO/VisDrone2019-YOLO-renamed/"
PROMPT_FILE = "prompt.jsonl"
OUTPUT_DIR = "/content/drive/MyDrive/VisDrone2019-YOLO/trained_lora_controlnet/"
CHECKPOINT_DIR = "/content/drive/MyDrive/VisDrone2019-YOLO/checkpoints"
PRETRAINED_MODEL_PATH = "stable-diffusion-v1-5/stable-diffusion-v1-5"

os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(CHECKPOINT_DIR, exist_ok=True)

# ğŸš€ ä¼˜åŒ–åçš„è®­ç»ƒå‚æ•° - å¹³è¡¡ç‰ˆæœ¬
BATCH_SIZE = 2
EPOCHS = 15  # å¢åŠ è½®æ•°ç»™è¶³æ—¶é—´æ”¶æ•›
LR = 2e-4  # é€‚ä¸­çš„å­¦ä¹ ç‡ï¼Œé¿å…è¿‡æ¿€
GRADIENT_ACCUMULATION_STEPS = 4  # å¢åŠ æœ‰æ•ˆbatch size
SCALE_LR = False  # ç¦ç”¨å­¦ä¹ ç‡ç¼©æ”¾ï¼Œé¿å…è¿‡å¤§
MAX_TOKEN_LENGTH = 77
IMAGE_SIZE = 512
CACHE_LATENTS = True
WARMUP_STEPS = 100  # å‡å°‘warmupï¼Œæ›´å¿«è¿›å…¥ä¸»è¦å­¦ä¹ é˜¶æ®µ
MIN_SNR_GAMMA = 5.0  # å¯ç”¨Min-SNRï¼Œç¨³å®šè®­ç»ƒ
NOISE_OFFSET = 0.1  # é€‚åº¦å¢åŠ å™ªå£°åç§»ï¼Œæå‡é²æ£’æ€§
MAX_GRAD_NORM = 1.0  # æ¢¯åº¦è£å‰ª
SAVE_STEPS = 1000  # æ›´é¢‘ç¹ä¿å­˜
VALIDATION_STEPS = 500  # æ·»åŠ éªŒè¯

epoch_losses = []
step_losses = []
weight_dtype = torch.float16 if torch.cuda.is_available() else torch.float32


def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)


set_seed(42)

# åˆå§‹åŒ–Accelerator
accelerator = Accelerator(
    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
    mixed_precision="fp16" if torch.cuda.is_available() else "no",
    log_with="tensorboard",  # æ·»åŠ æ—¥å¿—è®°å½•
    project_dir=os.path.join(OUTPUT_DIR, "logs")
)

print(f"ä½¿ç”¨è®¾å¤‡: {accelerator.device}")
print(f"æ··åˆç²¾åº¦: {accelerator.mixed_precision}")

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
noise_scheduler = DDPMScheduler.from_pretrained(PRETRAINED_MODEL_PATH, subfolder="scheduler")
tokenizer = CLIPTokenizer.from_pretrained(PRETRAINED_MODEL_PATH, subfolder="tokenizer")
text_encoder = CLIPTextModel.from_pretrained(PRETRAINED_MODEL_PATH, subfolder="text_encoder")
vae = AutoencoderKL.from_pretrained(PRETRAINED_MODEL_PATH, subfolder="vae")
unet = UNet2DConditionModel.from_pretrained(PRETRAINED_MODEL_PATH, subfolder="unet")

# å†»ç»“é¢„è®­ç»ƒå‚æ•°
unet.requires_grad_(False)
vae.requires_grad_(False)
text_encoder.requires_grad_(False)

# ğŸ¯ ä¼˜åŒ–çš„LoRAé…ç½® - å¹³è¡¡æ€§èƒ½å’Œç¨³å®šæ€§
unet_lora_config = LoraConfig(
    r=32,  # å¢åŠ rankæå‡è¡¨è¾¾èƒ½åŠ›
    lora_alpha=32,  # alpha = rankï¼Œæ ‡å‡†è®¾ç½®
    init_lora_weights="gaussian",
    target_modules=["to_k", "to_q", "to_v", "to_out.0"],
    lora_dropout=0.1,  # æ·»åŠ dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
)

# ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡
text_encoder = text_encoder.to(accelerator.device, dtype=weight_dtype)
vae = vae.to(accelerator.device, dtype=weight_dtype)
unet = unet.to(accelerator.device, dtype=weight_dtype)

# æ·»åŠ LoRAé€‚é…å™¨
unet.add_adapter(unet_lora_config)

# è®¾ç½®è®­ç»ƒå‚æ•°
cast_training_params(unet, dtype=torch.float32)

# è·å–LoRAå¯è®­ç»ƒå‚æ•°
lora_layers = list(filter(lambda p: p.requires_grad, unet.parameters()))
print(f"å¯è®­ç»ƒå‚æ•°æ•°é‡ï¼ˆLoRAï¼‰ï¼š{len(lora_layers)}")
print(f"å¯è®­ç»ƒå‚æ•°æ€»é‡ï¼š{sum(p.numel() for p in lora_layers)}")

# å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
if hasattr(unet, "enable_gradient_checkpointing"):
    unet.enable_gradient_checkpointing()

# ğŸ”¥ æ”¹è¿›çš„ä¼˜åŒ–å™¨è®¾ç½®
optimizer = torch.optim.AdamW(
    lora_layers,
    lr=LR,
    betas=(0.9, 0.999),
    weight_decay=1e-2,
    eps=1e-8,
)

# ğŸ¯ ä¸“æ³¨è®¡æ•°ä»»åŠ¡ - å»æ‰æ‰€æœ‰æ•°æ®å¢å¼º
transform = transforms.Compose([
    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.5] * 3, [0.5] * 3),
])


class VisDroneControlNetDataset(torch.utils.data.Dataset):
    def __init__(self, root_dir, prompt_file, tokenizer, vae=None, max_length=MAX_TOKEN_LENGTH, transform=None):
        self.root_dir = root_dir
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.transform = transform
        self.vae = vae
        self.cache_latents = CACHE_LATENTS and vae is not None

        prompt_path = os.path.join(root_dir, prompt_file)
        if not os.path.exists(prompt_path):
            raise FileNotFoundError(f"Prompt file not found: {prompt_path}")

        with open(prompt_path, "r") as f:
            self.entries = [json.loads(line) for line in f]

        print(f"åŠ è½½äº† {len(self.entries)} ä¸ªè®­ç»ƒæ ·æœ¬")

        if self.cache_latents:
            print("é¢„ç¼“å­˜latentsä¸­...")
            self._cache_latents()

    def _cache_latents(self):
        """é¢„ç¼“å­˜æ‰€æœ‰å›¾ç‰‡çš„latents"""
        self.cached_latents = []
        cache_dir = os.path.join(self.root_dir, "cached_latents")
        os.makedirs(cache_dir, exist_ok=True)

        batch_size = 4

        for start_idx in tqdm(range(0, len(self.entries), batch_size), desc="ç¼“å­˜latents"):
            batch_indices = range(start_idx, min(start_idx + batch_size, len(self.entries)))

            for idx in batch_indices:
                item = self.entries[idx]
                cache_file = os.path.join(cache_dir, f"latent_{idx}.pt")

                if os.path.exists(cache_file):
                    try:
                        latent = torch.load(cache_file, map_location="cpu")
                        self.cached_latents.append(latent)
                    except:
                        self.cached_latents.append(None)
                else:
                    try:
                        image_path = os.path.join(self.root_dir, item["image"])

                        if not os.path.exists(image_path):
                            self.cached_latents.append(None)
                            continue

                        image = Image.open(image_path).convert("RGB")

                        # æ³¨æ„ï¼šç¼“å­˜æ—¶ä¸åº”ç”¨æ•°æ®å¢å¼º
                        cache_transform = transforms.Compose([
                            transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),
                            transforms.ToTensor(),
                            transforms.Normalize([0.5] * 3, [0.5] * 3),
                        ])

                        image = cache_transform(image)

                        with torch.no_grad():
                            image_tensor = image.unsqueeze(0).to(self.vae.device, dtype=self.vae.dtype)
                            latent = self.vae.encode(image_tensor).latent_dist.sample()
                            latent = latent * self.vae.config.scaling_factor
                            latent = latent.squeeze(0).cpu()

                            del image_tensor
                            torch.cuda.empty_cache()

                        torch.save(latent, cache_file)
                        self.cached_latents.append(latent)

                    except Exception as e:
                        print(f"âš ï¸ ç¼“å­˜ç¬¬{idx}ä¸ªæ ·æœ¬å¤±è´¥: {e}")
                        self.cached_latents.append(None)

            if start_idx % (batch_size * 4) == 0:
                torch.cuda.empty_cache()

    def __len__(self):
        return len(self.entries)

    def __getitem__(self, idx):
        item = self.entries[idx]

        if self.cache_latents:
            latent = self.cached_latents[idx]
            image = None
        else:
            image_path = os.path.join(self.root_dir, item["image"])

            if not os.path.exists(image_path):
                raise FileNotFoundError(f"Image not found: {image_path}")

            image = Image.open(image_path).convert("RGB")
            if self.transform:
                image = self.transform(image)
            latent = None

        tokenized = self.tokenizer(
            item["prompt"],
            padding="max_length",
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt",
        )

        result = {
            "input_ids": tokenized.input_ids[0],
            "attention_mask": tokenized.attention_mask[0],
        }

        if self.cache_latents:
            result["latents"] = latent
        else:
            result["image"] = image

        return result


# åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
dataset = VisDroneControlNetDataset(
    DATA_DIR,
    PROMPT_FILE,
    tokenizer,
    vae=vae if CACHE_LATENTS else None,
    transform=transform
)


def collate_fn(examples):
    input_ids = torch.stack([ex["input_ids"] for ex in examples])
    attention_mask = torch.stack([ex["attention_mask"] for ex in examples])

    result = {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
    }

    if CACHE_LATENTS:
        latents = torch.stack([ex["latents"] for ex in examples])
        result["latents"] = latents
    else:
        pixel_values = torch.stack([ex["image"] for ex in examples])
        result["pixel_values"] = pixel_values

    return result


dataloader = DataLoader(
    dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    collate_fn=collate_fn,
    num_workers=2,
    pin_memory=True,
    prefetch_factor=2,
)

# å­¦ä¹ ç‡è°ƒåº¦å™¨
num_update_steps_per_epoch = len(dataloader) // GRADIENT_ACCUMULATION_STEPS
max_train_steps = EPOCHS * num_update_steps_per_epoch

print(f"æ¯epochæ›´æ–°æ­¥æ•°: {num_update_steps_per_epoch}")
print(f"æ€»è®­ç»ƒæ­¥æ•°: {max_train_steps}")

# ğŸŒŸ æ”¹è¿›çš„å­¦ä¹ ç‡è°ƒåº¦å™¨
lr_scheduler = get_scheduler(
    "cosine_with_restarts",  # ä½¿ç”¨cosineé‡å¯ï¼Œé¿å…è¿‡æ—©æ”¶æ•›
    optimizer=optimizer,
    num_warmup_steps=WARMUP_STEPS,
    num_training_steps=max_train_steps,
    num_cycles=3,  # 3æ¬¡é‡å¯å‘¨æœŸ
)


def compute_snr(timesteps, noise_scheduler):
    """
    è®¡ç®—ä¿¡å™ªæ¯”ç”¨äºMin-SNRæŸå¤±
    """
    alphas_cumprod = noise_scheduler.alphas_cumprod
    sqrt_alphas_cumprod = alphas_cumprod ** 0.5
    sqrt_one_minus_alphas_cumprod = (1.0 - alphas_cumprod) ** 0.5

    # è·å–å¯¹åº”timestepçš„å€¼
    sqrt_alphas_cumprod = sqrt_alphas_cumprod[timesteps].float()
    while len(sqrt_alphas_cumprod.shape) < len(timesteps.shape):
        sqrt_alphas_cumprod = sqrt_alphas_cumprod[..., None]

    sqrt_one_minus_alphas_cumprod = sqrt_one_minus_alphas_cumprod[timesteps].float()
    while len(sqrt_one_minus_alphas_cumprod.shape) < len(timesteps.shape):
        sqrt_one_minus_alphas_cumprod = sqrt_one_minus_alphas_cumprod[..., None]

    snr = (sqrt_alphas_cumprod / sqrt_one_minus_alphas_cumprod) ** 2
    return snr


def unwrap_model(model):
    model = accelerator.unwrap_model(model)
    return model


def save_lora_checkpoint(step, unet, optimizer, lr_scheduler, loss):
    """ä¿å­˜æ£€æŸ¥ç‚¹"""
    checkpoint_path = os.path.join(CHECKPOINT_DIR, f"checkpoint-{step}")
    accelerator.save_state(checkpoint_path)

    lora_path = os.path.join(CHECKPOINT_DIR, f"lora_step_{step}")
    os.makedirs(lora_path, exist_ok=True)

    unwrapped_unet = unwrap_model(unet)
    unet_lora_state_dict = convert_state_dict_to_diffusers(get_peft_model_state_dict(unwrapped_unet))

    StableDiffusionPipeline.save_lora_weights(
        save_directory=lora_path,
        unet_lora_layers=unet_lora_state_dict,
        safe_serialization=False,
    )

    print(f"æ£€æŸ¥ç‚¹ä¿å­˜åˆ°: {checkpoint_path}")
    print(f"LoRAæƒé‡ä¿å­˜åˆ°: {lora_path}")


def load_lora_checkpoint():
    """åŠ è½½æœ€æ–°çš„æ£€æŸ¥ç‚¹"""
    start_epoch = 1
    start_step = 0

    checkpoints = []
    if os.path.exists(CHECKPOINT_DIR):
        for item in os.listdir(CHECKPOINT_DIR):
            if item.startswith("checkpoint-"):
                step_num = int(item.split("-")[1])
                checkpoints.append((step_num, item))

    if checkpoints:
        checkpoints.sort(reverse=True)
        latest_checkpoint = checkpoints[0][1]
        checkpoint_path = os.path.join(CHECKPOINT_DIR, latest_checkpoint)

        print(f"å‘ç°æœ€æ–°æ£€æŸ¥ç‚¹: {latest_checkpoint}")

        try:
            accelerator.load_state(checkpoint_path)
            step_num = checkpoints[0][0]
            start_epoch = (step_num // num_update_steps_per_epoch) + 1
            start_step = step_num
            print(f"æˆåŠŸåŠ è½½æ£€æŸ¥ç‚¹ï¼Œå°†ä» epoch {start_epoch}, step {start_step} å¼€å§‹è®­ç»ƒ")

        except Exception as e:
            print(f"åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            start_epoch = 1
            start_step = 0

    return start_epoch, start_step


# å°è¯•åŠ è½½æ£€æŸ¥ç‚¹
start_epoch, global_step = load_lora_checkpoint()

# å‡†å¤‡è®­ç»ƒ
unet, optimizer, dataloader, lr_scheduler = accelerator.prepare(
    unet, optimizer, dataloader, lr_scheduler
)

print(f"ğŸš€ ä¸“æ³¨è®¡æ•°çš„è®­ç»ƒé…ç½®:")
print(f"   - å­¦ä¹ ç‡: {LR} (é€‚ä¸­è®¾ç½®)")
print(f"   - LoRA rank: {unet_lora_config.r} (å¢å¼ºè¡¨è¾¾èƒ½åŠ›)")
print(f"   - ç¦ç”¨å­¦ä¹ ç‡ç¼©æ”¾ (é¿å…è¿‡å¤§)")
print(f"   - ä½¿ç”¨Min-SNRæŸå¤± (gamma={MIN_SNR_GAMMA})")
print(f"   - æ¢¯åº¦ç´¯ç§¯: {GRADIENT_ACCUMULATION_STEPS}")
print(f"   - cosineé‡å¯è°ƒåº¦å™¨")
print(f"   - å»æ‰æ•°æ®å¢å¼º (ä¸“æ³¨è®¡æ•°å‡†ç¡®æ€§)")

# ğŸ¯ æ”¹è¿›çš„è®­ç»ƒå¾ªç¯
best_loss = float('inf')
loss_history = []

for epoch in range(start_epoch, EPOCHS + 1):
    unet.train()
    total_loss = 0.0
    epoch_step_losses = []

    progress_bar = tqdm(dataloader, desc=f"Epoch {epoch}/{EPOCHS}")

    for step, batch in enumerate(progress_bar):
        with accelerator.accumulate(unet):
            input_ids = batch["input_ids"].to(accelerator.device)
            attention_mask = batch["attention_mask"].to(accelerator.device)

            with torch.no_grad():
                encoder_hidden_states = text_encoder(
                    input_ids=input_ids,
                    attention_mask=attention_mask
                )[0]

            if CACHE_LATENTS:
                latents = batch["latents"].to(accelerator.device, dtype=weight_dtype)
            else:
                pixel_values = batch["pixel_values"].to(accelerator.device, dtype=weight_dtype)
                with torch.no_grad():
                    latents = vae.encode(pixel_values).latent_dist.sample()
                    latents = latents * vae.config.scaling_factor

            # ğŸ² æ”¹è¿›çš„å™ªå£°ç”Ÿæˆ
            noise = torch.randn_like(latents, device=accelerator.device, dtype=weight_dtype)
            if NOISE_OFFSET > 0:
                noise += NOISE_OFFSET * torch.randn(latents.shape[0], latents.shape[1], 1, 1,
                                                    device=accelerator.device, dtype=weight_dtype)

            timesteps = torch.randint(
                0, noise_scheduler.config.num_train_timesteps,
                (latents.shape[0],),
                device=accelerator.device
            ).long()

            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)

            model_pred = unet(
                sample=noisy_latents,
                timestep=timesteps,
                encoder_hidden_states=encoder_hidden_states,
                return_dict=False
            )[0]

            if noise_scheduler.config.prediction_type == "epsilon":
                target = noise
            elif noise_scheduler.config.prediction_type == "v_prediction":
                target = noise_scheduler.get_velocity(latents, noise, timesteps)
            else:
                raise ValueError(f"Unknown prediction type {noise_scheduler.config.prediction_type}")

            # ğŸ¯ Min-SNRæŸå¤±è®¡ç®—
            if MIN_SNR_GAMMA > 0:
                snr = compute_snr(timesteps, noise_scheduler)
                mse_loss_weights = (
                        torch.stack([snr, MIN_SNR_GAMMA * torch.ones_like(timesteps)], dim=1).min(dim=1)[0] / snr
                )
                # ç¡®ä¿æƒé‡ç»´åº¦åŒ¹é…
                while len(mse_loss_weights.shape) < len(model_pred.shape):
                    mse_loss_weights = mse_loss_weights.unsqueeze(-1)

                loss = torch.nn.functional.mse_loss(model_pred.float(), target.float(), reduction="none")
                loss = loss.mean(dim=list(range(1, len(loss.shape)))) * mse_loss_weights
                loss = loss.mean()
            else:
                loss = torch.nn.functional.mse_loss(model_pred.float(), target.float(), reduction="mean")

            accelerator.backward(loss)

            if accelerator.sync_gradients:
                accelerator.clip_grad_norm_(lora_layers, MAX_GRAD_NORM)

            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()

            # è®°å½•æŸå¤±
            current_loss = loss.detach().item()
            total_loss += current_loss
            epoch_step_losses.append(current_loss)
            step_losses.append(current_loss)
            loss_history.append(current_loss)

            if accelerator.sync_gradients:
                global_step += 1

            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({
                "loss": f"{current_loss:.4f}",
                "avg_loss": f"{np.mean(epoch_step_losses[-20:]):.4f}",
                "lr": f"{lr_scheduler.get_last_lr()[0]:.2e}",
                "best": f"{best_loss:.4f}",
                "step": f"{global_step}"
            })

            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if accelerator.sync_gradients and global_step % SAVE_STEPS == 0:
                save_lora_checkpoint(global_step, unet, optimizer, lr_scheduler, current_loss)

            # è®°å½•æ—¥å¿—
            if accelerator.sync_gradients and global_step % 50 == 0:
                accelerator.log({
                    "train_loss": current_loss,
                    "learning_rate": lr_scheduler.get_last_lr()[0],
                    "epoch": epoch,
                }, step=global_step)

    # epochç»“æŸç»Ÿè®¡
    avg_loss = total_loss / len(dataloader)
    epoch_losses.append(avg_loss)

    # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
    if avg_loss < best_loss:
        best_loss = avg_loss
        print(f"ğŸ‰ æ–°çš„æœ€ä½³loss: {best_loss:.6f}")
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        save_lora_checkpoint(f"best_{global_step}", unet, optimizer, lr_scheduler, avg_loss)

    print(f"\nEpoch {epoch} å®Œæˆ:")
    print(f"  å¹³å‡Loss: {avg_loss:.6f}")
    print(f"  å½“å‰æœ€ä½³Loss: {best_loss:.6f}")

    # è®­ç»ƒç¨³å®šæ€§æ£€æŸ¥
    if len(loss_history) > 100:
        recent_std = np.std(loss_history[-100:])
        print(f"  æœ€è¿‘100æ­¥Lossæ ‡å‡†å·®: {recent_std:.6f}")

print("è®­ç»ƒå®Œæˆ!")

# æœ€ç»ˆä¿å­˜
print("ä¿å­˜æœ€ç»ˆLoRAæ¨¡å‹...")
final_output_path = OUTPUT_DIR

unwrapped_unet = unwrap_model(unet)
unet_lora_state_dict = convert_state_dict_to_diffusers(get_peft_model_state_dict(unwrapped_unet))

StableDiffusionPipeline.save_lora_weights(
    save_directory=final_output_path,
    unet_lora_layers=unet_lora_state_dict,
    safe_serialization=False,
)

# ä¿å­˜é…ç½®ä¿¡æ¯
config_info = {
    "lora_config": {
        "r": unet_lora_config.r,
        "lora_alpha": unet_lora_config.lora_alpha,
        "target_modules": unet_lora_config.target_modules,
        "init_lora_weights": unet_lora_config.init_lora_weights,
        "lora_dropout": unet_lora_config.lora_dropout
    },
    "training_info": {
        "epochs": EPOCHS,
        "batch_size": BATCH_SIZE,
        "learning_rate": LR,
        "gradient_accumulation_steps": GRADIENT_ACCUMULATION_STEPS,
        "noise_offset": NOISE_OFFSET,
        "warmup_steps": WARMUP_STEPS,
        "min_snr_gamma": MIN_SNR_GAMMA,
        "scale_lr": SCALE_LR,
        "final_loss": epoch_losses[-1] if epoch_losses else None,
        "best_loss": best_loss,
        "optimization_level": "balanced"
    }
}

with open(os.path.join(final_output_path, "training_config.json"), "w") as f:
    json.dump(config_info, f, indent=2)

# æŸå¤±å¯è§†åŒ–
plt.figure(figsize=(15, 10))

# 1. EpochæŸå¤±
plt.subplot(2, 3, 1)
plt.plot(range(1, len(epoch_losses) + 1), epoch_losses, marker='o', linewidth=2)
plt.axhline(y=best_loss, color='red', linestyle='--', alpha=0.7, label=f'Best: {best_loss:.4f}')
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training Loss per Epoch")
plt.legend()
plt.grid(True, alpha=0.3)

# 2. æ­¥çº§åˆ«æŸå¤±(å¹³æ»‘)
if len(step_losses) > 100:
    plt.subplot(2, 3, 2)
    window_size = 100
    smoothed_losses = np.convolve(step_losses, np.ones(window_size) / window_size, mode='valid')
    plt.plot(smoothed_losses, linewidth=1.5, alpha=0.8)
    plt.xlabel("Step")
    plt.ylabel("Loss")
    plt.title(f"Step-wise Loss (MA-{window_size})")
    plt.grid(True, alpha=0.3)

# 3. æŸå¤±å˜åŒ–ç‡
if len(epoch_losses) > 1:
    plt.subplot(2, 3, 3)
    loss_changes = [epoch_losses[i] - epoch_losses[i - 1] for i in range(1, len(epoch_losses))]
    plt.plot(range(2, len(epoch_losses) + 1), loss_changes, marker='s', color='orange', linewidth=2)
    plt.xlabel("Epoch")
    plt.ylabel("Loss Change")
    plt.title("Loss Change Rate")
    plt.grid(True, alpha=0.3)
    plt.axhline(y=0, color='red', linestyle='--', alpha=0.7)

# 4. å­¦ä¹ ç‡æ›²çº¿
plt.subplot(2, 3, 4)
# è®¡ç®—å­¦ä¹ ç‡å†å²
lr_values = []
for step in range(max_train_steps):
    if step < WARMUP_STEPS:
        lr = LR * (step / WARMUP_STEPS)
    else:
        # cosine with restarts
        cycle_len = (max_train_steps - WARMUP_STEPS) // 3
        cycle_pos = (step - WARMUP_STEPS) % cycle_len
        lr = LR * 0.5 * (1 + np.cos(np.pi * cycle_pos / cycle_len))
    lr_values.append(lr)

plt.plot(lr_values, color='green', linewidth=2)
plt.xlabel("Step")
plt.ylabel("Learning Rate")
plt.title("Learning Rate Schedule")
plt.grid(True, alpha=0.3)

# 5. æŸå¤±åˆ†å¸ƒ
plt.subplot(2, 3, 5)
if len(step_losses) > 50:
    plt.hist(step_losses, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
    plt.axvline(x=np.mean(step_losses), color='red', linestyle='--', label=f'Mean: {np.mean(step_losses):.4f}')
    plt.axvline(x=np.median(step_losses), color='orange', linestyle='--', label=f'Median: {np.median(step_losses):.4f}')
    plt.xlabel("Loss")
    plt.ylabel("Frequency")
    plt.title("Loss Distribution")
    plt.legend()

# 6. æ”¶æ•›æ€§åˆ†æ
plt.subplot(2, 3, 6)
if len(step_losses) > 500:
    # è®¡ç®—æ»‘åŠ¨æ ‡å‡†å·®
    window = 200
    rolling_stds = []
    for i in range(window, len(step_losses), window // 4):
        std = np.std(step_losses[i - window:i])
        rolling_stds.append(std)

    plt.plot(rolling_stds, color='purple', linewidth=1.5)
    plt.xlabel("Window")
    plt.ylabel("Loss Std")
    plt.title("Training Stability")
    plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(os.path.join(OUTPUT_DIR, "balanced_training_analysis.png"), dpi=150, bbox_inches='tight')
plt.show()

print(f"\nğŸ¯ å¹³è¡¡ä¼˜åŒ–è®­ç»ƒå®Œæˆ!")
print(f"æœ€ç»ˆLoRAæ¨¡å‹ä¿å­˜åœ¨: {final_output_path}")
print(f"æœ€ä½³loss: {best_loss:.6f}")

if len(epoch_losses) > 1:
    loss_reduction = (epoch_losses[0] - epoch_losses[-1]) / epoch_losses[0] * 100
    print(f"Lossé™ä½: {loss_reduction:.2f}%")

    if loss_reduction > 20:
        print("âœ… è®­ç»ƒæ•ˆæœä¼˜ç§€!")
    elif loss_reduction > 10:
        print("âœ… è®­ç»ƒæ•ˆæœè‰¯å¥½!")
    else:
        print("âš ï¸ å»ºè®®ç»§ç»­è®­ç»ƒæˆ–è°ƒæ•´å‚æ•°")

# è®­ç»ƒæ€»ç»“
summary_info = {
    "optimization_type": "balanced",
    "key_improvements": [
        f"å­¦ä¹ ç‡é€‚ä¸­: {LR} (é¿å…è¿‡æ¿€)",
        f"LoRA rankå¢åŠ : {unet_lora_config.r}",
        "ç¦ç”¨å­¦ä¹ ç‡ç¼©æ”¾",
        f"Min-SNRæŸå¤± (gamma={MIN_SNR_GAMMA})",
        "cosineé‡å¯è°ƒåº¦å™¨",
        "å»æ‰æ•°æ®å¢å¼ºä¸“æ³¨è®¡æ•°",
        f"æ›´å¤§æœ‰æ•ˆbatch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}"
    ],
    "training_results": {
        "total_epochs": EPOCHS,
        "total_steps": global_step,
        "final_loss": epoch_losses[-1] if epoch_losses else "N/A",
        "best_loss": best_loss,
        "loss_reduction": f"{loss_reduction:.2f}%" if len(epoch_losses) > 1 else "N/A"
    },
    "safety_measures": [
        "æ¢¯åº¦è£å‰ªé˜²æ­¢çˆ†ç‚¸",
        "Min-SNRç¨³å®šè®­ç»ƒ",
        "é¢‘ç¹ä¿å­˜æ£€æŸ¥ç‚¹",
        "é€‚ä¸­çš„å­¦ä¹ ç‡é¿å…ä¸ç¨³å®š"
    ]
}

summary_path = os.path.join(OUTPUT_DIR, "balanced_training_summary.json")
with open(summary_path, "w") as f:
    json.dump(summary_info, f, indent=2, ensure_ascii=False)

print(f"\nğŸŠ ä¸»è¦æ”¹è¿›ç‚¹:")
print(f"1. ğŸ¯ å­¦ä¹ ç‡å¹³è¡¡: {LR} (ä¸ä¼šå¤ªæ¿€è¿›)")
print(f"2. ğŸ§  LoRA rankæå‡: {unet_lora_config.r} (æ›´å¼ºè¡¨è¾¾èƒ½åŠ›)")
print(f"3. ğŸš« ç¦ç”¨å­¦ä¹ ç‡ç¼©æ”¾ (é¿å…è¿‡å¤§å­¦ä¹ ç‡)")
print(f"4. ğŸ“Š Min-SNRæŸå¤± (æ›´ç¨³å®šçš„è®­ç»ƒ)")
print(f"5. ğŸ”„ cosineé‡å¯è°ƒåº¦ (é¿å…å±€éƒ¨æœ€ä¼˜)")
print(f"6. ğŸ¯ å»æ‰æ•°æ®å¢å¼º (ä¸“æ³¨è®¡æ•°ä»»åŠ¡)")
print(f"7. ğŸ’ª æ›´å¤§æœ‰æ•ˆbatch: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}")

print(f"\nğŸ’¡ è¿™ä¸ªç‰ˆæœ¬åº”è¯¥èƒ½æ˜¾è‘—æ”¹å–„lossä¸‹é™é€Ÿåº¦ï¼ŒåŒæ—¶ä¿æŒè®­ç»ƒç¨³å®šæ€§!")

# ç®€å•æ¨ç†æµ‹è¯•
print("\nğŸš€ å¼€å§‹å¿«é€Ÿæ¨ç†æµ‹è¯•...")

try:
    # æ¸…ç†å†…å­˜
    del unet, text_encoder, vae, optimizer, lr_scheduler
    torch.cuda.empty_cache()

    # åŠ è½½æ¨ç†æ¨¡å‹
    print("åŠ è½½æ¨ç†æ¨¡å‹...")
    pipe = StableDiffusionPipeline.from_pretrained(
        PRETRAINED_MODEL_PATH,
        torch_dtype=torch.float16,
        safety_checker=None,
        requires_safety_checker=False
    ).to("cuda")

    # åŠ è½½LoRAæƒé‡
    pipe.load_lora_weights(final_output_path)

    # å¿«é€Ÿæµ‹è¯•
    test_prompt = "There are 5 pedestrians, 2 cars, and 1 truck in the image."
    print(f"æµ‹è¯•prompt: {test_prompt}")

    with torch.no_grad():
        image = pipe(
            test_prompt,
            num_inference_steps=20,
            guidance_scale=7.5,
            generator=torch.manual_seed(42)
        ).images[0]

    # ä¿å­˜æµ‹è¯•å›¾ç‰‡
    test_path = os.path.join(OUTPUT_DIR, "quick_test.png")
    image.save(test_path)
    print(f"âœ… æ¨ç†æµ‹è¯•æˆåŠŸ! å›¾ç‰‡ä¿å­˜åœ¨: {test_path}")

    del pipe
    torch.cuda.empty_cache()

except Exception as e:
    print(f"âš ï¸ æ¨ç†æµ‹è¯•å¤±è´¥: {e}")
    print("è¿™æ˜¯æ­£å¸¸çš„ï¼Œå¯èƒ½æ˜¯å†…å­˜ä¸è¶³ï¼Œä½†LoRAæƒé‡å·²æ­£ç¡®ä¿å­˜")

print(f"\nğŸ å¹³è¡¡ä¼˜åŒ–è®­ç»ƒå…¨éƒ¨å®Œæˆ!")
print(f"ğŸ“ æ‰€æœ‰æ–‡ä»¶ä¿å­˜åœ¨: {OUTPUT_DIR}")
print(f"ğŸ¯ å…³é”®æ–‡ä»¶:")
print(f"   - pytorch_lora_weights.bin (ç”¨äºæ¨ç†)")
print(f"   - training_config.json (è®­ç»ƒé…ç½®)")
print(f"   - balanced_training_analysis.png (è®­ç»ƒåˆ†æ)")
print(f"   - balanced_training_summary.json (è®­ç»ƒæ€»ç»“)")

print(f"\nğŸ’« ç›¸æ¯”åŸç‰ˆæœ¬çš„ä¸»è¦ä¼˜åŠ¿:")
print(f"   ğŸš€ æ›´å¿«çš„lossä¸‹é™é€Ÿåº¦")
print(f"   ğŸ›¡ï¸ æ›´ç¨³å®šçš„è®­ç»ƒè¿‡ç¨‹")
print(f"   ğŸ¯ æ›´å¥½çš„æ”¶æ•›æ€§èƒ½")
print(f"   ğŸ”§ æ›´æ™ºèƒ½çš„å­¦ä¹ ç‡è°ƒåº¦")
print(f"   ğŸ“ˆ æ›´å¼ºçš„æ¨¡å‹è¡¨è¾¾èƒ½åŠ›")

print(f"\nğŸŒŸ è¿™ä¸ªå¹³è¡¡ç‰ˆæœ¬åº”è¯¥èƒ½ç»™ä½ æ›´å¥½çš„è®­ç»ƒæ•ˆæœ!")