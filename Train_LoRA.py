import os
import json
from PIL import Image
import torch
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
from transformers import CLIPTokenizer
from diffusers import (
    StableDiffusionControlNetPipeline,
    ControlNetModel,
    UniPCMultistepScheduler,
)
from diffusers.models.attention_processor import LoRAAttnProcessor, LoRAAttnProcessor2_0

# === é…ç½®å‚æ•° ===
DATA_DIR = "/content/drive/MyDrive/VisDrone2019-YOLO/VisDrone2019-YOLO-train/"
PROMPT_FILE = "prompt.jsonl"
OUTPUT_DIR = "/content/drive/MyDrive/VisDrone2019-YOLO/trained_lora_controlnet/"
os.makedirs(OUTPUT_DIR, exist_ok=True)

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
BATCH_SIZE = 1
EPOCHS = 50
LR = 1e-4
MAX_TOKEN_LENGTH = 77
IMAGE_SIZE = 512

# === åŠ è½½ ControlNet å’Œ Pipeline ===
controlnet = ControlNetModel.from_pretrained(
    "lllyasviel/control_v11p_sd15_scribble", torch_dtype=torch.float16
)
pipe = StableDiffusionControlNetPipeline.from_pretrained(
    "stable-diffusion-v1-5/stable-diffusion-v1-5",
    controlnet=controlnet,
    torch_dtype=torch.float16,
)
pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
pipe = pipe.to(DEVICE)
pipe.enable_model_cpu_offload()

# === æ³¨å…¥ LoRA æ³¨æ„åŠ›å¤„ç†å™¨ ===
for name in pipe.unet.attn_processors.keys():
    pipe.unet.attn_processors[name] = LoRAAttnProcessor()

for name in pipe.controlnet.attn_processors.keys():
    pipe.controlnet.attn_processors[name] = LoRAAttnProcessor()
    print("ğŸ” type(pipe.unet.attn_processors):", type(pipe.unet.attn_processors))
    print("ğŸ”‘ Keys in attn_processors:", list(pipe.unet.attn_processors.keys()))

    # éšä¾¿æŒ‘ä¸€ä¸ª key æŸ¥çœ‹å†…å®¹
    sample_key = list(pipe.unet.attn_processors.keys())[0]
    print(f"ğŸ” Type of processor at '{sample_key}':", type(pipe.unet.attn_processors[sample_key]))

# è®¾ç½®å¯è®­ç»ƒå‚æ•°ï¼Œåªè®­ç»ƒ LoRA å±‚å’Œæ–‡æœ¬ç¼–ç å™¨
for param in pipe.unet.parameters():
    param.requires_grad = False
for param in pipe.controlnet.parameters():
    param.requires_grad = False


pipe.text_encoder.train()
for param in pipe.text_encoder.parameters():
    param.requires_grad = True

pipe.unet.train()
pipe.controlnet.train()

# === åŠ è½½ Tokenizer ===
tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-large-patch14")

# === è‡ªå®šä¹‰æ•°æ®é›† ===
class VisDroneControlNetDataset(Dataset):
    def __init__(self, root_dir, prompt_file, tokenizer, max_length=MAX_TOKEN_LENGTH):
        self.root_dir = root_dir
        self.tokenizer = tokenizer
        self.max_length = max_length
        with open(os.path.join(root_dir, prompt_file), "r") as f:
            self.entries = [json.loads(line) for line in f]

    def __len__(self):
        return len(self.entries)

    def __getitem__(self, idx):
        item = self.entries[idx]
        image_path = os.path.join(self.root_dir, item["image_path"])
        layout_path = os.path.join(self.root_dir, item["layout_path"])
        prompt = item["prompt"]

        image = Image.open(image_path).convert("RGB").resize((IMAGE_SIZE, IMAGE_SIZE))
        layout = Image.open(layout_path).convert("RGB").resize((IMAGE_SIZE, IMAGE_SIZE))

        # ä½¿ç”¨pipeçš„feature_extractorå¤„ç†å›¾åƒå’Œå¸ƒå±€
        image_tensor = pipe.feature_extractor(images=image, return_tensors="pt").pixel_values[0]
        layout_tensor = pipe.feature_extractor(images=layout, return_tensors="pt").pixel_values[0]

        tokenized = self.tokenizer(
            prompt,
            padding="max_length",
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt",
        )

        return {
            "image": image_tensor,
            "layout": layout_tensor,
            "input_ids": tokenized.input_ids[0],
            "attention_mask": tokenized.attention_mask[0],
        }

dataset = VisDroneControlNetDataset(DATA_DIR, PROMPT_FILE, tokenizer)
dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

# === ä¼˜åŒ–å™¨ï¼ˆåªè®­ç»ƒ LoRA å’Œ text_encoderï¼‰ ===
def get_lora_parameters(attn_procs):
    params = []
    for name, proc in attn_procs.items():
        if isinstance(proc, (LoRAAttnProcessor, LoRAAttnProcessor2_0)):
            params.extend(proc.parameters())
    return params

optimizer = torch.optim.AdamW(
    get_lora_parameters(pipe.unet.attn_processors) +
    get_lora_parameters(pipe.controlnet.attn_processors) +
    list(pipe.text_encoder.parameters()),
    lr=LR,
)

# === è®­ç»ƒå¾ªç¯ ===
for epoch in range(EPOCHS):
    pipe.unet.train()
    pipe.controlnet.train()
    pipe.text_encoder.train()

    loop = tqdm(dataloader, desc=f"Epoch {epoch + 1}/{EPOCHS}")
    for batch in loop:
        optimizer.zero_grad()

        # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
        image = batch["image"].to(DEVICE, dtype=torch.float16)
        layout = batch["layout"].to(DEVICE, dtype=torch.float16)
        input_ids = batch["input_ids"].to(DEVICE)
        attention_mask = batch["attention_mask"].to(DEVICE)

        # ç¼–ç æ–‡æœ¬
        encoder_hidden_states = pipe.text_encoder(input_ids=input_ids, attention_mask=attention_mask)[0].to(dtype=torch.float16).to(DEVICE)

        # ç¼–ç å›¾åƒè‡³latent
        latents = pipe.vae.encode(image).latent_dist.sample().to(DEVICE)
        latents = latents * pipe.vae.config.scaling_factor
        latents = latents.to(dtype=torch.float16)

        print("pipe device:", next(pipe.parameters()).device)
        print("image device:", image.device)
        print("layout device:", layout.device)
        print("latents device:", latents.device)
        print("encoder_hidden_states device:", encoder_hidden_states.device)
        print("controlnet_cond device:", layout.device)  # ä½ ä¼ ç»™controlnetçš„cond

        # é‡‡æ ·éšæœºæ—¶é—´æ­¥
        timesteps = torch.randint(0, pipe.scheduler.config.num_train_timesteps, (latents.shape[0],), device=DEVICE).long()

        # æ·»åŠ å™ªå£°
        noise = torch.randn_like(latents).to(DEVICE)
        noisy_latents = pipe.scheduler.add_noise(latents, noise, timesteps)

        # ControlNetå‰å‘
        controlnet_out = pipe.controlnet(
            sample=noisy_latents,
            timestep=timesteps,
            encoder_hidden_states=encoder_hidden_states,
            controlnet_cond=layout,
            return_dict=True,
        )

        # UNetå‰å‘ï¼ŒèåˆControlNetè¾“å‡º
        unet_out = pipe.unet(
            sample=noisy_latents,
            timestep=timesteps,
            encoder_hidden_states=encoder_hidden_states,
            down_block_additional_residuals=controlnet_out.down_block_res_samples,
            mid_block_additional_residual=controlnet_out.mid_block_res_sample,
            return_dict=True,
        )

        # é¢„æµ‹å™ªå£°
        noise_pred = unet_out.sample

        # è®¡ç®—æŸå¤±
        loss = torch.nn.functional.mse_loss(noise_pred, noise)

        loss.backward()
        optimizer.step()

        loop.set_postfix(loss=loss.item())

# ä¿å­˜æ¨¡å‹LoRAæƒé‡å’Œæ–‡æœ¬ç¼–ç å™¨
pipe.unet.save_attn_procs(os.path.join(OUTPUT_DIR, "unet_lora"))
pipe.controlnet.save_attn_procs(os.path.join(OUTPUT_DIR, "controlnet_lora"))
pipe.text_encoder.save_pretrained(os.path.join(OUTPUT_DIR, "text_encoder"))

print("è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹ä¿å­˜å®Œæ¯•ã€‚")
