import os
import json
import torch
from accelerate import Accelerator
from PIL import Image
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
from torchvision import transforms
import random
import numpy as np

from diffusers import (
    UNet2DConditionModel,
    AutoencoderKL,
    DDPMScheduler,
    StableDiffusionPipeline,
)
from diffusers.training_utils import cast_training_params
from diffusers.optimization import get_scheduler
from diffusers.utils import convert_state_dict_to_diffusers
from transformers import CLIPTextModel, CLIPTokenizer
from peft import LoraConfig, get_peft_model_state_dict

DATA_DIR = "/content/drive/MyDrive/VisDrone2019-YOLO/VisDrone2019-YOLO-renamed/"
PROMPT_FILE = "prompt.jsonl"
OUTPUT_DIR = "/content/drive/MyDrive/VisDrone2019-YOLO/trained_lora_controlnet/"
CHECKPOINT_DIR = "/content/drive/MyDrive/VisDrone2019-YOLO/checkpoints"
PRETRAINED_MODEL_PATH = "stable-diffusion-v1-5/stable-diffusion-v1-5"

os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(CHECKPOINT_DIR, exist_ok=True)

# ğŸ”§ ä¿å®ˆçš„ä¼˜åŒ–å‚æ•° - é€æ­¥æå‡
BATCH_SIZE = 2
EPOCHS = 12
LR = 2e-4  # ğŸ”§ é€‚åº¦æé«˜ï¼š1e-4 â†’ 2e-4 (åªæé«˜1å€)
GRADIENT_ACCUMULATION_STEPS = 3  # ğŸ”§ é€‚åº¦å¢åŠ ï¼š2 â†’ 3
SCALE_LR = True  # ğŸ”§ ä¿æŒåŸæœ‰çš„å­¦ä¹ ç‡ç¼©æ”¾
MAX_TOKEN_LENGTH = 77
IMAGE_SIZE = 512
CACHE_LATENTS = True
WARMUP_STEPS = 300  # ğŸ”§ é€‚åº¦å‡å°‘ï¼š500 â†’ 300
MIN_SNR_GAMMA = 0  # ğŸ”§ å…ˆä¸ç”¨Min-SNRï¼Œé¿å…è¿‡äºå¤æ‚
NOISE_OFFSET = 0.05  # ğŸ”§ å¾ˆå°çš„å™ªå£°åç§»ï¼Œå‡ ä¹ä¸å½±å“è®­ç»ƒ

epoch_losses = []
step_losses = []
weight_dtype = torch.float16 if torch.cuda.is_available() else torch.float32


# è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡å¤æ€§
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)


set_seed(42)

# åˆå§‹åŒ–Accelerator
accelerator = Accelerator(
    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
    mixed_precision="fp16" if torch.cuda.is_available() else "no"
)

print(f"ä½¿ç”¨è®¾å¤‡: {accelerator.device}")
print(f"æ··åˆç²¾åº¦: {accelerator.mixed_precision}")

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
noise_scheduler = DDPMScheduler.from_pretrained(PRETRAINED_MODEL_PATH, subfolder="scheduler")
tokenizer = CLIPTokenizer.from_pretrained(PRETRAINED_MODEL_PATH, subfolder="tokenizer")
text_encoder = CLIPTextModel.from_pretrained(PRETRAINED_MODEL_PATH, subfolder="text_encoder")
vae = AutoencoderKL.from_pretrained(PRETRAINED_MODEL_PATH, subfolder="vae")
unet = UNet2DConditionModel.from_pretrained(PRETRAINED_MODEL_PATH, subfolder="unet")

# å†»ç»“é¢„è®­ç»ƒå‚æ•°
unet.requires_grad_(False)
vae.requires_grad_(False)
text_encoder.requires_grad_(False)

# ğŸ”§ é€‚åº¦ä¼˜åŒ–çš„LoRAé…ç½®
unet_lora_config = LoraConfig(
    r=24,  # ğŸ”§ é€‚åº¦å¢åŠ ï¼š16 â†’ 24
    lora_alpha=32,  # ğŸ”§ ä¿æŒä¸å˜ï¼Œé¿å…è¿‡å¤§å½±å“
    init_lora_weights="gaussian",
    target_modules=["to_k", "to_q", "to_v", "to_out.0"],
)

# ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡
text_encoder = text_encoder.to(accelerator.device, dtype=weight_dtype)
vae = vae.to(accelerator.device, dtype=weight_dtype)
unet = unet.to(accelerator.device, dtype=weight_dtype)

# æ·»åŠ LoRAé€‚é…å™¨
unet.add_adapter(unet_lora_config)

# è®¾ç½®è®­ç»ƒå‚æ•°
cast_training_params(unet, dtype=torch.float32)

# è·å–LoRAå¯è®­ç»ƒå‚æ•°
lora_layers = list(filter(lambda p: p.requires_grad, unet.parameters()))
print(f"å¯è®­ç»ƒå‚æ•°æ•°é‡ï¼ˆLoRAï¼‰ï¼š{len(lora_layers)}")
print(f"å¯è®­ç»ƒå‚æ•°æ€»é‡ï¼š{sum(p.numel() for p in lora_layers)}")

# éªŒè¯LoRAå‚æ•°
print("\néƒ¨åˆ†LoRAå‚æ•°ç¤ºä¾‹ï¼š")
count = 0
for name, param in unet.named_parameters():
    if param.requires_grad and count < 5:
        print(f"  {name}: shape={param.shape}, requires_grad={param.requires_grad}")
        count += 1

# å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
if hasattr(unet, "enable_gradient_checkpointing"):
    unet.enable_gradient_checkpointing()

# ğŸ”§ ä¿å®ˆçš„ä¼˜åŒ–å™¨è®¾ç½®
optimizer = torch.optim.AdamW(
    lora_layers,
    lr=LR,
    betas=(0.9, 0.999),
    weight_decay=1e-2,  # ğŸ”§ ä¿æŒåŸæœ‰è®¾ç½®
    eps=1e-8,
)

# å­¦ä¹ ç‡ç¼©æ”¾ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
if SCALE_LR:
    scaled_lr = LR * GRADIENT_ACCUMULATION_STEPS * BATCH_SIZE * accelerator.num_processes
    print(f"åŸå§‹å­¦ä¹ ç‡: {LR}")
    print(f"ç¼©æ”¾åå­¦ä¹ ç‡: {scaled_lr}")
    # é‡æ–°åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        lora_layers,
        lr=scaled_lr,
        betas=(0.9, 0.999),
        weight_decay=1e-2,
        eps=1e-8,
    )

# ğŸ”§ é’ˆå¯¹LoRAä¼˜åŒ–çš„æ•°æ®é¢„å¤„ç†
# ç§»é™¤æ•°æ®å¢å¼ºï¼Œä¸“æ³¨äºlossä¸‹é™
transform = transforms.Compose([
    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),
    # ğŸš« ç§»é™¤RandomHorizontalFlip - å¯èƒ½ç ´åèˆªæ‹å›¾åƒçš„ç©ºé—´è¯­ä¹‰
    # ğŸš« ç§»é™¤ColorJitter - å¯èƒ½å½±å“ç‰©ä½“è¯†åˆ«å’Œè®¡æ•°
    transforms.ToTensor(),
    transforms.Normalize([0.5] * 3, [0.5] * 3),
])


# å¯é€‰ï¼šå¦‚æœæƒ³è¦è½»å¾®çš„é²æ£’æ€§ï¼Œå¯ä»¥æ·»åŠ å¾ˆå°çš„å™ªå£°
# transform = transforms.Compose([
#     transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),
#     transforms.ToTensor(),
#     transforms.Lambda(lambda x: x + torch.randn_like(x) * 0.01),  # å¾ˆå°çš„å™ªå£°
#     transforms.Normalize([0.5]*3, [0.5]*3),
# ])

class VisDroneControlNetDataset(torch.utils.data.Dataset):
    def __init__(self, root_dir, prompt_file, tokenizer, vae=None, max_length=MAX_TOKEN_LENGTH, transform=None):
        self.root_dir = root_dir
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.transform = transform
        self.vae = vae
        self.cache_latents = CACHE_LATENTS and vae is not None

        prompt_path = os.path.join(root_dir, prompt_file)
        if not os.path.exists(prompt_path):
            raise FileNotFoundError(f"Prompt file not found: {prompt_path}")

        with open(prompt_path, "r") as f:
            self.entries = [json.loads(line) for line in f]

        print(f"åŠ è½½äº† {len(self.entries)} ä¸ªè®­ç»ƒæ ·æœ¬")

        if self.cache_latents:
            print("é¢„ç¼“å­˜latentsä¸­...")
            self._cache_latents()

    def _cache_latents(self):
        """é¢„ç¼“å­˜æ‰€æœ‰å›¾ç‰‡çš„latents"""
        self.cached_latents = []
        cache_dir = os.path.join(self.root_dir, "cached_latents")
        os.makedirs(cache_dir, exist_ok=True)

        batch_size = 4

        for start_idx in tqdm(range(0, len(self.entries), batch_size), desc="ç¼“å­˜latents"):
            batch_indices = range(start_idx, min(start_idx + batch_size, len(self.entries)))

            for idx in batch_indices:
                item = self.entries[idx]
                cache_file = os.path.join(cache_dir, f"latent_{idx}.pt")

                if os.path.exists(cache_file):
                    try:
                        latent = torch.load(cache_file, map_location="cpu")
                        self.cached_latents.append(latent)
                    except:
                        self.cached_latents.append(None)
                else:
                    try:
                        image_path = os.path.join(self.root_dir, item["image"])

                        if not os.path.exists(image_path):
                            self.cached_latents.append(None)
                            continue

                        image = Image.open(image_path).convert("RGB")

                        if self.transform:
                            image = self.transform(image)

                        with torch.no_grad():
                            image_tensor = image.unsqueeze(0).to(self.vae.device, dtype=self.vae.dtype)
                            latent = self.vae.encode(image_tensor).latent_dist.sample()
                            latent = latent * self.vae.config.scaling_factor
                            latent = latent.squeeze(0).cpu()

                            del image_tensor
                            torch.cuda.empty_cache()

                        torch.save(latent, cache_file)
                        self.cached_latents.append(latent)

                    except Exception as e:
                        print(f"âš ï¸ ç¼“å­˜ç¬¬{idx}ä¸ªæ ·æœ¬å¤±è´¥: {e}")
                        self.cached_latents.append(None)

            if start_idx % (batch_size * 4) == 0:
                torch.cuda.empty_cache()

    def __len__(self):
        return len(self.entries)

    def __getitem__(self, idx):
        item = self.entries[idx]

        if self.cache_latents:
            latent = self.cached_latents[idx]
            image = None
        else:
            image_path = os.path.join(self.root_dir, item["image"])

            if not os.path.exists(image_path):
                raise FileNotFoundError(f"Image not found: {image_path}")

            image = Image.open(image_path).convert("RGB")
            if self.transform:
                image = self.transform(image)
            latent = None

        tokenized = self.tokenizer(
            item["prompt"],
            padding="max_length",
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt",
        )

        result = {
            "input_ids": tokenized.input_ids[0],
            "attention_mask": tokenized.attention_mask[0],
        }

        if self.cache_latents:
            result["latents"] = latent
        else:
            result["image"] = image

        return result


# åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
dataset = VisDroneControlNetDataset(
    DATA_DIR,
    PROMPT_FILE,
    tokenizer,
    vae=vae if CACHE_LATENTS else None,
    transform=transform
)


def collate_fn(examples):
    input_ids = torch.stack([ex["input_ids"] for ex in examples])
    attention_mask = torch.stack([ex["attention_mask"] for ex in examples])

    result = {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
    }

    if CACHE_LATENTS:
        latents = torch.stack([ex["latents"] for ex in examples])
        result["latents"] = latents
    else:
        pixel_values = torch.stack([ex["image"] for ex in examples])
        result["pixel_values"] = pixel_values

    return result


dataloader = DataLoader(
    dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    collate_fn=collate_fn,
    num_workers=2,
    pin_memory=False,
    prefetch_factor=1,
)

# å­¦ä¹ ç‡è°ƒåº¦å™¨
num_update_steps_per_epoch = len(dataloader) // GRADIENT_ACCUMULATION_STEPS
max_train_steps = EPOCHS * num_update_steps_per_epoch

print(f"æ¯epochæ›´æ–°æ­¥æ•°: {num_update_steps_per_epoch}")
print(f"æ€»è®­ç»ƒæ­¥æ•°: {max_train_steps}")

# ğŸ”§ ä¿æŒcosineè°ƒåº¦å™¨ï¼Œä½†è°ƒæ•´warmup
lr_scheduler = get_scheduler(
    "cosine",
    optimizer=optimizer,
    num_warmup_steps=WARMUP_STEPS,  # å‡å°‘äº†warmupæ­¥æ•°
    num_training_steps=max_train_steps,
)


def unwrap_model(model):
    model = accelerator.unwrap_model(model)
    return model


def save_lora_checkpoint(epoch, unet, optimizer, lr_scheduler, loss):
    """ä¿å­˜æ£€æŸ¥ç‚¹"""
    checkpoint_path = os.path.join(CHECKPOINT_DIR, f"checkpoint-{epoch * len(dataloader)}")
    accelerator.save_state(checkpoint_path)

    lora_path = os.path.join(CHECKPOINT_DIR, f"lora_epoch_{epoch}")
    os.makedirs(lora_path, exist_ok=True)

    unwrapped_unet = unwrap_model(unet)
    unet_lora_state_dict = convert_state_dict_to_diffusers(get_peft_model_state_dict(unwrapped_unet))

    StableDiffusionPipeline.save_lora_weights(
        save_directory=lora_path,
        unet_lora_layers=unet_lora_state_dict,
        safe_serialization=False,
    )

    print(f"å®Œæ•´æ£€æŸ¥ç‚¹ä¿å­˜åˆ°: {checkpoint_path}")
    print(f"LoRAæƒé‡ä¿å­˜åˆ°: {lora_path}")


def load_lora_checkpoint():
    """åŠ è½½æœ€æ–°çš„æ£€æŸ¥ç‚¹"""
    start_epoch = 1

    checkpoints = []
    if os.path.exists(CHECKPOINT_DIR):
        for item in os.listdir(CHECKPOINT_DIR):
            if item.startswith("checkpoint-"):
                step_num = int(item.split("-")[1])
                checkpoints.append((step_num, item))

    if checkpoints:
        checkpoints.sort(reverse=True)
        latest_checkpoint = checkpoints[0][1]
        checkpoint_path = os.path.join(CHECKPOINT_DIR, latest_checkpoint)

        print(f"å‘ç°æœ€æ–°æ£€æŸ¥ç‚¹: {latest_checkpoint}")

        try:
            accelerator.load_state(checkpoint_path)
            step_num = checkpoints[0][0]
            start_epoch = (step_num // len(dataloader)) + 1
            print(f"æˆåŠŸåŠ è½½æ£€æŸ¥ç‚¹ï¼Œå°†ä» epoch {start_epoch} å¼€å§‹è®­ç»ƒ")

        except Exception as e:
            print(f"åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            start_epoch = 1

    return start_epoch


# å°è¯•åŠ è½½æ£€æŸ¥ç‚¹
start_epoch = load_lora_checkpoint()

# å‡†å¤‡è®­ç»ƒ
unet, optimizer, dataloader, lr_scheduler = accelerator.prepare(
    unet, optimizer, dataloader, lr_scheduler
)


def get_lora_param_stats(model):
    """è·å–LoRAå‚æ•°çš„ç»Ÿè®¡ä¿¡æ¯"""
    lora_params = []
    for name, param in model.named_parameters():
        if param.requires_grad and any(lora_key in name for lora_key in ["lora_A", "lora_B"]):
            lora_params.append(param.detach().cpu())

    if lora_params:
        all_params = torch.cat([p.flatten() for p in lora_params])
        return {
            "mean": all_params.mean().item(),
            "std": all_params.std().item(),
            "min": all_params.min().item(),
            "max": all_params.max().item()
        }
    return None


print(f"ğŸ”§ ä¿å®ˆä¼˜åŒ–çš„è®­ç»ƒé…ç½®:")
print(f"   - å­¦ä¹ ç‡é€‚åº¦æé«˜: 1e-4 â†’ {LR}")
print(f"   - LoRA ranké€‚åº¦å¢åŠ : 16 â†’ {unet_lora_config.r}")
print(f"   - æ¢¯åº¦ç´¯ç§¯é€‚åº¦å¢åŠ : 2 â†’ {GRADIENT_ACCUMULATION_STEPS}")
print(f"   - å‡å°‘warmupæ­¥æ•°: 500 â†’ {WARMUP_STEPS}")
print(f"   - ä¿æŒåŸæœ‰å­¦ä¹ ç‡ç¼©æ”¾å’Œè°ƒåº¦å™¨")
print(f"   - æ·»åŠ æå°çš„å™ªå£°åç§»: {NOISE_OFFSET}")

# ğŸ”§ æ”¹è¿›çš„è®­ç»ƒå¾ªç¯ - å¢åŠ ç›‘æ§ä½†ä¸æ”¹å˜æ ¸å¿ƒé€»è¾‘
global_step = 0
best_loss = float('inf')
loss_patience = 0  # ç”¨äºæ—©åœçš„è€å¿ƒè®¡æ•°

for epoch in range(start_epoch, EPOCHS + 1):
    unet.train()
    total_loss = 0.0
    epoch_step_losses = []

    lora_stats_before = get_lora_param_stats(unet)
    if lora_stats_before:
        print(f"\nEpoch {epoch} å¼€å§‹ - LoRAå‚æ•°ç»Ÿè®¡:")
        print(f"  å‡å€¼: {lora_stats_before['mean']:.6f}")
        print(f"  æ ‡å‡†å·®: {lora_stats_before['std']:.6f}")

    progress_bar = tqdm(dataloader, desc=f"Epoch {epoch}/{EPOCHS}")

    for step, batch in enumerate(progress_bar):
        with accelerator.accumulate(unet):
            input_ids = batch["input_ids"].to(accelerator.device)
            attention_mask = batch["attention_mask"].to(accelerator.device)

            with torch.no_grad():
                encoder_hidden_states = text_encoder(
                    input_ids=input_ids,
                    attention_mask=attention_mask
                )[0]

            if CACHE_LATENTS:
                latents = batch["latents"].to(accelerator.device, dtype=weight_dtype)
            else:
                pixel_values = batch["pixel_values"].to(accelerator.device, dtype=weight_dtype)
                with torch.no_grad():
                    latents = vae.encode(pixel_values).latent_dist.sample()
                    latents = latents * vae.config.scaling_factor

            # ğŸ”§ ä¿å®ˆçš„å™ªå£°å¤„ç†
            noise = torch.randn_like(latents, device=accelerator.device, dtype=weight_dtype)
            if NOISE_OFFSET > 0:
                # æ·»åŠ å¾ˆå°çš„å™ªå£°åç§»
                noise += NOISE_OFFSET * torch.randn(latents.shape[0], latents.shape[1], 1, 1, device=accelerator.device,
                                                    dtype=weight_dtype)

            timesteps = torch.randint(
                0, noise_scheduler.config.num_train_timesteps,
                (latents.shape[0],),
                device=accelerator.device
            ).long()

            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)

            model_pred = unet(
                sample=noisy_latents,
                timestep=timesteps,
                encoder_hidden_states=encoder_hidden_states,
                return_dict=False
            )[0]

            if noise_scheduler.config.prediction_type == "epsilon":
                target = noise
            elif noise_scheduler.config.prediction_type == "v_prediction":
                target = noise_scheduler.get_velocity(latents, noise, timesteps)
            else:
                raise ValueError(f"Unknown prediction type {noise_scheduler.config.prediction_type}")

            # ğŸ”§ ä¿æŒåŸæœ‰çš„ç®€å•MSEæŸå¤±
            loss = torch.nn.functional.mse_loss(model_pred.float(), target.float(), reduction="mean")

            accelerator.backward(loss)

            if accelerator.sync_gradients:
                accelerator.clip_grad_norm_(lora_layers, 1.0)

            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()

            # è®°å½•æŸå¤±
            current_loss = loss.detach().item()
            total_loss += current_loss
            epoch_step_losses.append(current_loss)
            step_losses.append(current_loss)
            global_step += 1

            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({
                "loss": f"{current_loss:.4f}",
                "avg_loss": f"{np.mean(epoch_step_losses[-20:]):.4f}",  # æœ€è¿‘20æ­¥çš„å¹³å‡loss
                "lr": f"{lr_scheduler.get_last_lr()[0]:.2e}",
                "step": f"{step}/{len(dataloader)}"
            })

    # epochç»“æŸç»Ÿè®¡
    avg_loss = total_loss / len(dataloader)
    epoch_losses.append(avg_loss)

    # ğŸ”§ æ”¹è¿›çš„æŸå¤±åˆ†æ
    loss_std = np.std(epoch_step_losses)
    recent_loss_trend = np.mean(epoch_step_losses[-100:]) if len(epoch_step_losses) > 100 else avg_loss

    lora_stats_after = get_lora_param_stats(unet)
    if lora_stats_after and lora_stats_before:
        mean_change = abs(lora_stats_after['mean'] - lora_stats_before['mean'])
        std_change = abs(lora_stats_after['std'] - lora_stats_before['std'])

        print(f"\nEpoch {epoch} ç»“æŸ:")
        print(f"  å¹³å‡Loss: {avg_loss:.6f}")
        print(f"  Lossæ ‡å‡†å·®: {loss_std:.6f}")
        print(f"  æœ€å100æ­¥å‡å€¼: {recent_loss_trend:.6f}")
        print(f"  LoRAå‚æ•°å‡å€¼å˜åŒ–: {mean_change:.8f}")
        print(f"  LoRAå‚æ•°æ ‡å‡†å·®å˜åŒ–: {std_change:.8f}")

        # ğŸ”§ å¥åº·æ£€æŸ¥ - ç»™å‡ºå…·ä½“å»ºè®®
        if mean_change < 1e-7:
            print(f"  âš ï¸  LoRAå‚æ•°å˜åŒ–æå° (<1e-7)ï¼Œå¯èƒ½éœ€è¦:")
            print(f"      - é€‚åº¦å¢åŠ å­¦ä¹ ç‡ (å½“å‰: {lr_scheduler.get_last_lr()[0]:.2e})")
            print(f"      - æ£€æŸ¥æ¢¯åº¦æ˜¯å¦æ­£å¸¸")
        elif mean_change > 1e-3:
            print(f"  âš ï¸  LoRAå‚æ•°å˜åŒ–è¿‡å¤§ (>1e-3)ï¼Œè€ƒè™‘:")
            print(f"      - é™ä½å­¦ä¹ ç‡")
            print(f"      - å¢åŠ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°")
        else:
            print(f"  âœ… LoRAå‚æ•°å˜åŒ–åœ¨åˆç†èŒƒå›´å†…")

        # æ—©åœæ£€æŸ¥
        if avg_loss < best_loss:
            best_loss = avg_loss
            loss_patience = 0
        else:
            loss_patience += 1

        if loss_patience >= 3:
            print(f"  ğŸ“Š Lossè¿ç»­3ä¸ªepochæœªæ”¹å–„ï¼Œå¯èƒ½æ¥è¿‘æ”¶æ•›")

    # ä¿å­˜æ£€æŸ¥ç‚¹
    if epoch % 4 == 0 or epoch == EPOCHS:
        save_lora_checkpoint(epoch, unet, optimizer, lr_scheduler, avg_loss)

print("è®­ç»ƒå®Œæˆ!")

# æœ€ç»ˆä¿å­˜
print("ä¿å­˜æœ€ç»ˆLoRAæ¨¡å‹...")
final_output_path = OUTPUT_DIR

unwrapped_unet = unwrap_model(unet)
unet_lora_state_dict = convert_state_dict_to_diffusers(get_peft_model_state_dict(unwrapped_unet))

StableDiffusionPipeline.save_lora_weights(
    save_directory=final_output_path,
    unet_lora_layers=unet_lora_state_dict,
    safe_serialization=False,
)

# ä¿å­˜é…ç½®ä¿¡æ¯
config_info = {
    "lora_config": {
        "r": unet_lora_config.r,
        "lora_alpha": unet_lora_config.lora_alpha,
        "target_modules": unet_lora_config.target_modules,
        "init_lora_weights": unet_lora_config.init_lora_weights
    },
    "training_info": {
        "epochs": EPOCHS,
        "batch_size": BATCH_SIZE,
        "learning_rate": LR,
        "gradient_accumulation_steps": GRADIENT_ACCUMULATION_STEPS,
        "noise_offset": NOISE_OFFSET,
        "warmup_steps": WARMUP_STEPS,
        "final_loss": epoch_losses[-1] if epoch_losses else None,
        "best_loss": best_loss,
        "optimization_level": "conservative"
    }
}

with open(os.path.join(final_output_path, "training_config.json"), "w") as f:
    json.dump(config_info, f, indent=2)

# ğŸ”§ æ”¹è¿›ä½†ä¿å®ˆçš„losså¯è§†åŒ–
plt.figure(figsize=(15, 8))

# 1. Epochçº§åˆ«çš„loss
plt.subplot(2, 3, 1)
plt.plot(range(1, len(epoch_losses) + 1), epoch_losses, marker='o', label="Training Loss", linewidth=2)
if len(epoch_losses) > 1:
    # æ·»åŠ è¶‹åŠ¿çº¿
    z = np.polyfit(range(1, len(epoch_losses) + 1), epoch_losses, 1)
    p = np.poly1d(z)
    plt.plot(range(1, len(epoch_losses) + 1), p(range(1, len(epoch_losses) + 1)), "--", alpha=0.7, label="Trend")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training Loss per Epoch")
plt.legend()
plt.grid(True, alpha=0.3)

# 2. æ­¥çº§åˆ«çš„lossï¼ˆæ»‘åŠ¨å¹³å‡ï¼‰
if len(step_losses) > 50:
    plt.subplot(2, 3, 2)
    window_size = 50
    smoothed_losses = np.convolve(step_losses, np.ones(window_size) / window_size, mode='valid')
    plt.plot(smoothed_losses, label=f"Smoothed Loss", linewidth=1.5, alpha=0.8)
    plt.xlabel("Step")
    plt.ylabel("Loss")
    plt.title(f"Step-wise Loss (MA-{window_size})")
    plt.legend()
    plt.grid(True, alpha=0.3)

# 3. Losså˜åŒ–ç‡
if len(epoch_losses) > 1:
    plt.subplot(2, 3, 3)
    loss_changes = [epoch_losses[i] - epoch_losses[i - 1] for i in range(1, len(epoch_losses))]
    plt.plot(range(2, len(epoch_losses) + 1), loss_changes, marker='s', color='orange', linewidth=2)
    plt.xlabel("Epoch")
    plt.ylabel("Loss Change")
    plt.title("Loss Change Rate")
    plt.grid(True, alpha=0.3)
    plt.axhline(y=0, color='red', linestyle='--', alpha=0.7)

# 4. å­¦ä¹ ç‡æ›²çº¿
plt.subplot(2, 3, 4)
lr_values = []
# æ¨¡æ‹Ÿå­¦ä¹ ç‡å˜åŒ–
for step in range(max_train_steps):
    if step < WARMUP_STEPS:
        lr = LR * GRADIENT_ACCUMULATION_STEPS * BATCH_SIZE * accelerator.num_processes * (step / WARMUP_STEPS)
    else:
        progress = (step - WARMUP_STEPS) / (max_train_steps - WARMUP_STEPS)
        lr = LR * GRADIENT_ACCUMULATION_STEPS * BATCH_SIZE * accelerator.num_processes * 0.5 * (
                    1 + np.cos(np.pi * progress))
    lr_values.append(lr)

plt.plot(lr_values, label="Learning Rate", linewidth=2, color='green')
plt.xlabel("Step")
plt.ylabel("Learning Rate")
plt.title("Learning Rate Schedule")
plt.legend()
plt.grid(True, alpha=0.3)

# 5. è®­ç»ƒç¨³å®šæ€§åˆ†æ
plt.subplot(2, 3, 5)
if len(step_losses) > 200:
    # è®¡ç®—æ¯100æ­¥çš„æ ‡å‡†å·®
    window = 100
    stds = []
    for i in range(window, len(step_losses), window // 2):
        std = np.std(step_losses[i - window:i])
        stds.append(std)

    plt.plot(stds, label="Loss Stability", color='purple', linewidth=1.5)
    plt.xlabel("Window")
    plt.ylabel("Loss Std")
    plt.title("Training Stability")
    plt.legend()
    plt.grid(True, alpha=0.3)

# 6. æ”¶æ•›åˆ†æ
plt.subplot(2, 3, 6)
if len(epoch_losses) > 3:
    # è®¡ç®—ç›¸å¯¹æ”¹å–„
    improvements = []
    for i in range(1, len(epoch_losses)):
        if epoch_losses[i - 1] > 0:
            improvement = (epoch_losses[i - 1] - epoch_losses[i]) / epoch_losses[i - 1] * 100
            improvements.append(improvement)

    plt.bar(range(2, len(epoch_losses) + 1), improvements, alpha=0.7, color='skyblue')
    plt.xlabel("Epoch")
    plt.ylabel("Improvement (%)")
    plt.title("Loss Improvement per Epoch")
    plt.grid(True, alpha=0.3)
    plt.axhline(y=0, color='red', linestyle='--', alpha=0.7)

plt.tight_layout()
plt.savefig(os.path.join(OUTPUT_DIR, "conservative_training_analysis.png"), dpi=150, bbox_inches='tight')
plt.show()

print(f"æœ€ç»ˆLoRAæ¨¡å‹ä¿å­˜åœ¨: {final_output_path}")
print(f"ä¸»è¦æ–‡ä»¶:")
print(f"  - pytorch_lora_weights.bin (æœ€ç»ˆLoRAæƒé‡ï¼Œç”¨äºæ¨ç†)")
print(f"  - training_config.json (è®­ç»ƒé…ç½®)")
print(f"  - conservative_training_analysis.png (è®­ç»ƒåˆ†æ)")

print(f"\næ£€æŸ¥ç‚¹æ–‡ä»¶ä¿å­˜åœ¨: {CHECKPOINT_DIR}")
print(f"  - checkpoint-xxxx/ (å®Œæ•´è®­ç»ƒçŠ¶æ€ï¼Œç”¨äºæ–­ç‚¹ç»­è®­)")
print(f"  - lora_epoch_x/ (LoRAæƒé‡å¤‡ä»½)")

# ğŸ”§ ä¿å®ˆçš„è®­ç»ƒæ•ˆæœåˆ†æ
print("\n" + "=" * 60)
print("ğŸ“Š ä¿å®ˆä¼˜åŒ–è®­ç»ƒåˆ†æ")
print("=" * 60)

if len(epoch_losses) > 1:
    initial_loss = epoch_losses[0]
    final_loss = epoch_losses[-1]
    loss_reduction = (initial_loss - final_loss) / initial_loss * 100

    print(f"åˆå§‹Loss: {initial_loss:.6f}")
    print(f"æœ€ç»ˆLoss: {final_loss:.6f}")
    print(f"Lossé™ä½: {loss_reduction:.2f}%")
    print(f"æœ€ä½³Loss: {best_loss:.6f}")

    # æ›´ç»†è‡´çš„æ•ˆæœè¯„ä¼°
    if loss_reduction > 15:
        print("âœ… è®­ç»ƒæ•ˆæœä¼˜ç§€ï¼ŒLossæ˜¾è‘—ä¸‹é™")
    elif loss_reduction > 8:
        print("âœ… è®­ç»ƒæ•ˆæœè‰¯å¥½ï¼Œå»ºè®®ç»§ç»­è®­ç»ƒ")
    elif loss_reduction > 3:
        print("âš ï¸ è®­ç»ƒæ•ˆæœä¸€èˆ¬ï¼Œå¯è€ƒè™‘:")
        print("   - å»¶é•¿è®­ç»ƒè½®æ•°")
        print("   - é€‚åº¦æé«˜å­¦ä¹ ç‡")
        print("   - å¢åŠ LoRA rank")
    else:
        print("âŒ è®­ç»ƒæ•ˆæœä¸ä½³ï¼Œå»ºè®®:")
        print("   - æ£€æŸ¥æ•°æ®è´¨é‡")
        print("   - æé«˜å­¦ä¹ ç‡ (å½“å‰è¾ƒä¿å®ˆ)")
        print("   - å¢åŠ è®­ç»ƒè½®æ•°")

if len(step_losses) > 100:
    recent_loss_std = np.std(step_losses[-100:])
    overall_loss_std = np.std(step_losses)

    print(f"\næ”¶æ•›æ€§åˆ†æ:")
    print(f"  æœ€è¿‘100æ­¥Lossæ ‡å‡†å·®: {recent_loss_std:.6f}")
    print(f"  æ•´ä½“Lossæ ‡å‡†å·®: {overall_loss_std:.6f}")

    if recent_loss_std < overall_loss_std * 0.5:
        print("  âœ… è®­ç»ƒå·²åŸºæœ¬æ”¶æ•›")
    elif recent_loss_std < overall_loss_std * 0.8:
        print("  âš ï¸ è®­ç»ƒæ¥è¿‘æ”¶æ•›ï¼Œå¯é€‚å½“å»¶é•¿")
    else:
        print("  âŒ è®­ç»ƒå°šæœªå……åˆ†æ”¶æ•›ï¼Œå»ºè®®ç»§ç»­")

print(f"\nå‚æ•°å˜åŒ–åˆ†æ:")
print(f"  LoRA rank: {unet_lora_config.r} (é€‚åº¦æå‡)")
print(f"  å­¦ä¹ ç‡: {LR} (ä¿å®ˆæå‡)")
print(f"  æœ‰æ•ˆbatch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}")

print("\n" + "=" * 60)
print("ğŸ¨ å¼€å§‹æ¨ç†æµ‹è¯•...")
print("=" * 60)

# æ¸…ç†GPUå†…å­˜
del unet, text_encoder, vae, optimizer, lr_scheduler
torch.cuda.empty_cache()

try:
    # åŠ è½½æ¨ç†pipeline
    print("æ­£åœ¨åŠ è½½æ¨ç†æ¨¡å‹...")
    inference_pipeline = StableDiffusionPipeline.from_pretrained(
        PRETRAINED_MODEL_PATH,
        torch_dtype=torch.float16,
        safety_checker=None,
        requires_safety_checker=False
    ).to("cuda")

    # åŠ è½½è®­ç»ƒå¥½çš„LoRAæƒé‡
    print("æ­£åœ¨åŠ è½½LoRAæƒé‡...")
    inference_pipeline.load_lora_weights(final_output_path)

    # åŸºç¡€çš„æ¨ç†è®¾ç½®
    if hasattr(inference_pipeline, 'enable_memory_efficient_attention'):
        inference_pipeline.enable_memory_efficient_attention()

    # æµ‹è¯•prompts - æ¶µç›–ä¸åŒå¤æ‚åº¦
    test_prompts = [
        "There are 5 pedestrians, 1 van, and 2 trucks in the image.",
        "There are 10 pedestrians, 3 cars, and 1 bus in the image.",
        "There are 2 pedestrians, 4 cars, and 2 motorcycles in the image.",
        "There are 15 pedestrians, 2 vans, 1 truck, and 3 cars in the image.",
        "There are 8 pedestrians, 1 van, and 5 cars in the image.",
        "There are 3 pedestrians and 1 motorcycle in the image.",
    ]

    # ç”Ÿæˆæµ‹è¯•å›¾ç‰‡
    inference_results_dir = os.path.join(OUTPUT_DIR, "inference_results")
    os.makedirs(inference_results_dir, exist_ok=True)

    print(f"æ­£åœ¨ç”Ÿæˆ {len(test_prompts)} å¼ æµ‹è¯•å›¾ç‰‡...")

    # ä½¿ç”¨æ ‡å‡†æ¨ç†è®¾ç½®
    inference_config = {"steps": 25, "guidance": 7.5}

    for i, prompt in enumerate(test_prompts):
        print(f"ç”Ÿæˆå›¾ç‰‡ {i + 1}/{len(test_prompts)}: {prompt[:40]}...")

        # ç”Ÿæˆå›¾ç‰‡
        with torch.no_grad():
            image = inference_pipeline(
                prompt,
                num_inference_steps=inference_config["steps"],
                guidance_scale=inference_config["guidance"],
                generator=torch.manual_seed(42 + i)
            ).images[0]

        # ä¿å­˜å›¾ç‰‡
        image_path = os.path.join(inference_results_dir, f"test_image_{i + 1}.png")
        image.save(image_path)

        # ä¿å­˜promptä¿¡æ¯
        prompt_info = {
            "image": f"test_image_{i + 1}.png",
            "prompt": prompt,
            "inference_steps": inference_config["steps"],
            "guidance_scale": inference_config["guidance"],
            "seed": 42 + i
        }

        info_path = os.path.join(inference_results_dir, f"test_image_{i + 1}_info.json")
        with open(info_path, "w") as f:
            json.dump(prompt_info, f, indent=2)

    # åˆ›å»ºç»“æœæ€»è§ˆå›¾
    print("åˆ›å»ºç»“æœæ€»è§ˆ...")
    from PIL import Image as PILImage

    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('Conservative LoRA Training Results', fontsize=16, fontweight='bold')

    for i, prompt in enumerate(test_prompts):
        row = i // 3
        col = i % 3

        image_path = os.path.join(inference_results_dir, f"test_image_{i + 1}.png")
        img = PILImage.open(image_path)

        axes[row, col].imshow(img)
        axes[row, col].set_title(f"Test {i + 1}", fontsize=12, fontweight='bold')

        # è‡ªåŠ¨æ¢è¡Œæ˜¾ç¤ºprompt
        words = prompt.split()
        lines = []
        current_line = []
        char_count = 0

        for word in words:
            if char_count + len(word) + 1 <= 50:
                current_line.append(word)
                char_count += len(word) + 1
            else:
                lines.append(' '.join(current_line))
                current_line = [word]
                char_count = len(word)

        if current_line:
            lines.append(' '.join(current_line))

        prompt_text = '\n'.join(lines)

        axes[row, col].text(0.02, 0.98, prompt_text, transform=axes[row, col].transAxes,
                            fontsize=9, verticalalignment='top',
                            bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))
        axes[row, col].axis('off')

    plt.tight_layout()
    overview_path = os.path.join(inference_results_dir, "results_overview.png")
    plt.savefig(overview_path, dpi=150, bbox_inches='tight')
    plt.close()

    print("\n" + "=" * 60)
    print("ğŸ‰ æ¨ç†æµ‹è¯•å®Œæˆï¼")
    print("=" * 60)
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {inference_results_dir}")
    print(f"ğŸ–¼ï¸  ç”Ÿæˆäº† {len(test_prompts)} å¼ æµ‹è¯•å›¾ç‰‡")
    print(f"ğŸ“Š ç»“æœæ€»è§ˆ: results_overview.png")

    print(f"\nğŸ” æ•ˆæœè¯„ä¼°å»ºè®®:")
    print(f"æ£€æŸ¥ç”Ÿæˆçš„å›¾ç‰‡æ˜¯å¦:")
    print(f"  âœ“ åŒ…å«æ­£ç¡®çš„ç‰©ä½“ç±»å‹")
    print(f"  âœ“ æ•°é‡å¤§è‡´å‡†ç¡®")
    print(f"  âœ“ å…·æœ‰èˆªæ‹è§†è§’é£æ ¼")
    print(f"  âœ“ ç”»é¢æ¸…æ™°è‡ªç„¶")

    # æ ¹æ®è®­ç»ƒlossç»™å‡ºè¿›ä¸€æ­¥å»ºè®®
    if len(epoch_losses) > 1:
        loss_reduction = (epoch_losses[0] - epoch_losses[-1]) / epoch_losses[0] * 100
        if loss_reduction < 5:
            print(f"\nğŸ’¡ è¿›ä¸€æ­¥ä¼˜åŒ–å»ºè®® (å½“å‰Lossé™ä½: {loss_reduction:.1f}%):")
            print(f"  - è€ƒè™‘å¢åŠ è®­ç»ƒè½®æ•°åˆ° {EPOCHS + 5}-{EPOCHS + 10}")
            print(f"  - é€‚åº¦æé«˜å­¦ä¹ ç‡åˆ° 3e-4")
            print(f"  - å¢åŠ LoRA rankåˆ° 32")
            print(f"  - æ£€æŸ¥æ•°æ®é›†è´¨é‡å’ŒpromptåŒ¹é…åº¦")

    del inference_pipeline
    torch.cuda.empty_cache()

except Exception as e:
    print(f"âŒ æ¨ç†æµ‹è¯•å¤±è´¥: {e}")
    print("å»ºè®®æ£€æŸ¥:")
    print("  - GPUå†…å­˜ä½¿ç”¨æƒ…å†µ")
    print("  - LoRAæƒé‡æ–‡ä»¶å®Œæ•´æ€§")
    print("  - åŸºç¡€æ¨¡å‹åŠ è½½æ˜¯å¦æ­£å¸¸")

    error_info = {
        "error": str(e),
        "error_type": type(e).__name__,
        "optimization_level": "conservative",
        "suggestion": "æ£€æŸ¥èµ„æºå’Œæ–‡ä»¶å®Œæ•´æ€§"
    }

    error_path = os.path.join(OUTPUT_DIR, "inference_error.json")
    with open(error_path, "w") as f:
        json.dump(error_info, f, indent=2)

# æœ€ç»ˆæ€»ç»“
print("\n" + "ğŸ”§" * 20)
print("ä¿å®ˆä¼˜åŒ–è®­ç»ƒå®Œæˆæ€»ç»“")
print("ğŸ”§" * 20)

summary_info = {
    "optimization_type": "conservative",
    "changes_made": [
        f"å­¦ä¹ ç‡: 1e-4 â†’ {LR} (2å€æå‡)",
        f"LoRA rank: 16 â†’ {unet_lora_config.r} (1.5å€æå‡)",
        f"æ¢¯åº¦ç´¯ç§¯: 2 â†’ {GRADIENT_ACCUMULATION_STEPS}",
        f"warmupæ­¥æ•°: 500 â†’ {WARMUP_STEPS}",
        f"æ·»åŠ è½»å¾®å™ªå£°åç§»: {NOISE_OFFSET}",
        "ä¿æŒåŸæœ‰è°ƒåº¦å™¨å’Œç¼©æ”¾é€»è¾‘"
    ],
    "training_results": {
        "total_epochs": EPOCHS,
        "final_loss": epoch_losses[-1] if epoch_losses else "N/A",
        "best_loss": best_loss,
        "loss_reduction": f"{((epoch_losses[0] - epoch_losses[-1]) / epoch_losses[0] * 100):.2f}%" if len(
            epoch_losses) > 1 else "N/A"
    },
    "safety_measures": [
        "ä¿æŒå­¦ä¹ ç‡ç¼©æ”¾é¿å…è¿‡å¤§å­¦ä¹ ç‡",
        "ä½¿ç”¨cosineè°ƒåº¦å™¨å¹³æ»‘å­¦ä¹ ",
        "ä¿æŒåŸæœ‰çš„MSEæŸå¤±å‡½æ•°",
        "é€‚åº¦çš„å‚æ•°è°ƒæ•´é¿å…ä¸ç¨³å®š"
    ]
}

summary_path = os.path.join(OUTPUT_DIR, "conservative_training_summary.json")
with open(summary_path, "w") as f:
    json.dump(summary_info, f, indent=2, ensure_ascii=False)

print(f"ğŸ“Š ä¿å®ˆä¼˜åŒ–æ‘˜è¦:")
print(f"   ç±»å‹: æ¸è¿›å¼ä¿å®ˆä¼˜åŒ–")
print(f"   é£é™©: ä½ (ä¿æŒåŸæœ‰æ ¸å¿ƒé€»è¾‘)")
print(f"   é¢„æœŸ: é€‚åº¦æ”¹å–„è®­ç»ƒæ•ˆæœ")

if len(epoch_losses) > 1:
    loss_reduction = (epoch_losses[0] - epoch_losses[-1]) / epoch_losses[0] * 100
    print(f"   å®é™…Lossé™ä½: {loss_reduction:.2f}%")

    if loss_reduction > 5:
        print(f"   âœ… ä¼˜åŒ–æˆåŠŸï¼Œè¾¾åˆ°é¢„æœŸæ•ˆæœ")
    else:
        print(f"   âš ï¸ å¯è€ƒè™‘è¿›ä¸€æ­¥ä¼˜åŒ–:")
        print(f"      - å»¶é•¿è®­ç»ƒ (+3-5 epochs)")
        print(f"      - é€‚åº¦æé«˜å­¦ä¹ ç‡ (â†’ 3e-4)")
        print(f"      - å¢åŠ LoRA rank (â†’ 32)")

print(f"\nğŸ“ æ‰€æœ‰æ–‡ä»¶ä¿å­˜åœ¨: {OUTPUT_DIR}")
print(f"ğŸš€ ä¿å®ˆä¼˜åŒ–å®Œæˆï¼Œå¯å®‰å…¨ä½¿ç”¨ï¼")

print("\nğŸŒ™ ä¿å®ˆä¼˜åŒ–è®­ç»ƒå®Œæˆï¼")
print("è¿™ä¸ªç‰ˆæœ¬åº”è¯¥ä¸ä¼šå‡ºç°lossçˆ†ç‚¸çš„é—®é¢˜ï½")