import os
import json
import torch
from accelerate import Accelerator
from PIL import Image
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
from torchvision import transforms
import random
import numpy as np
import time

from diffusers import (
    UNet2DConditionModel,
    AutoencoderKL,
    DDPMScheduler,
    StableDiffusionPipeline,
)
from diffusers.training_utils import cast_training_params
from diffusers.optimization import get_scheduler
from diffusers.utils import convert_state_dict_to_diffusers
from transformers import CLIPTextModel, CLIPTokenizer
from peft import LoraConfig, get_peft_model_state_dict

# é…ç½®è·¯å¾„
DATA_DIR = "/content/drive/MyDrive/VisDrone2019-YOLO/VisDrone2019-YOLO-renamed/"
PROMPT_FILE = "prompt.jsonl"
OUTPUT_DIR = "/content/drive/MyDrive/VisDrone2019-YOLO/trained_lora_controlnet/"
CHECKPOINT_DIR = "/content/drive/MyDrive/VisDrone2019-YOLO/checkpoints"
PRETRAINED_MODEL_PATH = "stable-diffusion-v1-5/stable-diffusion-v1-5"

os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(CHECKPOINT_DIR, exist_ok=True)

# ğŸš¨ ç´§æ€¥ä¿®å¤å‚æ•°
BATCH_SIZE = 2
EPOCHS = 15
LR = 1e-3  # å¤§å¹…æå‡å­¦ä¹ ç‡
GRADIENT_ACCUMULATION_STEPS = 1  # ç¦ç”¨æ¢¯åº¦ç´¯ç§¯
MAX_TOKEN_LENGTH = 77
IMAGE_SIZE = 512
MAX_GRAD_NORM = 10.0
SAVE_STEPS = 200

epoch_losses = []
step_losses = []
gradient_health_history = []
weight_dtype = torch.float32  # å¼ºåˆ¶float32


def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)


set_seed(42)

# åˆå§‹åŒ–Accelerator - ç¦ç”¨æ··åˆç²¾åº¦
accelerator = Accelerator(
    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
    mixed_precision="no",  # ç¦ç”¨æ··åˆç²¾åº¦
)

print(f"ğŸš¨ ç´§æ€¥ä¿®å¤é…ç½®:")
print(f"è®¾å¤‡: {accelerator.device}")
print(f"æ··åˆç²¾åº¦: {accelerator.mixed_precision}")
print(f"å­¦ä¹ ç‡: {LR}")

# åŠ è½½æ¨¡å‹
print("åŠ è½½é¢„è®­ç»ƒæ¨¡å‹...")
noise_scheduler = DDPMScheduler.from_pretrained(PRETRAINED_MODEL_PATH, subfolder="scheduler")
tokenizer = CLIPTokenizer.from_pretrained(PRETRAINED_MODEL_PATH, subfolder="tokenizer")
text_encoder = CLIPTextModel.from_pretrained(PRETRAINED_MODEL_PATH, subfolder="text_encoder")
vae = AutoencoderKL.from_pretrained(PRETRAINED_MODEL_PATH, subfolder="vae")
unet = UNet2DConditionModel.from_pretrained(PRETRAINED_MODEL_PATH, subfolder="unet")

# å†»ç»“æ¨¡å‹
unet.requires_grad_(False)
vae.requires_grad_(False)
text_encoder.requires_grad_(False)

# LoRAé…ç½®
unet_lora_config = LoraConfig(
    r=32,
    lora_alpha=32,
    init_lora_weights="gaussian",
    target_modules=["to_k", "to_q", "to_v", "to_out.0"],
    lora_dropout=0.0,
)

# ç§»åŠ¨åˆ°è®¾å¤‡
text_encoder = text_encoder.to(accelerator.device, dtype=weight_dtype)
vae = vae.to(accelerator.device, dtype=weight_dtype)
unet = unet.to(accelerator.device, dtype=weight_dtype)

# æ·»åŠ LoRA
unet.add_adapter(unet_lora_config)

# è·å–LoRAå‚æ•°
lora_layers = []
for name, param in unet.named_parameters():
    if param.requires_grad:
        lora_layers.append(param)

if not lora_layers:
    raise RuntimeError("æ²¡æœ‰æ‰¾åˆ°å¯è®­ç»ƒçš„LoRAå‚æ•°!")

print(f"æ‰¾åˆ° {len(lora_layers)} ä¸ªLoRAå‚æ•°")

# ä¼˜åŒ–å™¨
optimizer = torch.optim.AdamW(
    lora_layers,
    lr=LR,
    betas=(0.9, 0.999),
    weight_decay=1e-3,
    eps=1e-6,
)

# æ•°æ®å˜æ¢
transform = transforms.Compose([
    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.5] * 3, [0.5] * 3),
])


class VisDroneDataset(torch.utils.data.Dataset):
    def __init__(self, root_dir, prompt_file, tokenizer, transform=None):
        self.root_dir = root_dir
        self.tokenizer = tokenizer
        self.transform = transform

        prompt_path = os.path.join(root_dir, prompt_file)
        with open(prompt_path, "r") as f:
            self.entries = [json.loads(line) for line in f]

        print(f"åŠ è½½äº† {len(self.entries)} ä¸ªæ ·æœ¬")

    def __len__(self):
        return len(self.entries)

    def __getitem__(self, idx):
        item = self.entries[idx]

        image_path = os.path.join(self.root_dir, item["image"])
        image = Image.open(image_path).convert("RGB")
        if self.transform:
            image = self.transform(image)

        tokenized = self.tokenizer(
            item["prompt"],
            padding="max_length",
            truncation=True,
            max_length=MAX_TOKEN_LENGTH,
            return_tensors="pt",
        )

        return {
            "input_ids": tokenized.input_ids[0],
            "attention_mask": tokenized.attention_mask[0],
            "image": image,
        }


# åˆ›å»ºæ•°æ®é›†
dataset = VisDroneDataset(DATA_DIR, PROMPT_FILE, tokenizer, transform)


def collate_fn(examples):
    return {
        "input_ids": torch.stack([ex["input_ids"] for ex in examples]),
        "attention_mask": torch.stack([ex["attention_mask"] for ex in examples]),
        "pixel_values": torch.stack([ex["image"] for ex in examples]),
    }


dataloader = DataLoader(
    dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    collate_fn=collate_fn,
    num_workers=0,
)


def check_gradients(unet, step):
    """æ£€æŸ¥æ¢¯åº¦çŠ¶æ€"""
    total_grad_norm = 0
    valid_count = 0
    zero_count = 0

    for name, param in unet.named_parameters():
        if param.requires_grad:
            if param.grad is not None:
                grad_norm = param.grad.data.norm(2).item()
                total_grad_norm += grad_norm ** 2
                if grad_norm == 0:
                    zero_count += 1
                else:
                    valid_count += 1
            else:
                zero_count += 1

    total_grad_norm = total_grad_norm ** 0.5

    print(f"Step {step}: æ¢¯åº¦èŒƒæ•°={total_grad_norm:.6f}, æœ‰æ•ˆ={valid_count}, é›¶={zero_count}")

    if total_grad_norm == 0:
        return "critical"
    elif total_grad_norm < 1e-6:
        return "too_small"
    else:
        return "healthy"


# å‡†å¤‡è®­ç»ƒ
unet, optimizer, dataloader = accelerator.prepare(unet, optimizer, dataloader)

print(f"å¼€å§‹è®­ç»ƒ...")

best_loss = float('inf')
global_step = 0

for epoch in range(1, EPOCHS + 1):
    unet.train()
    total_loss = 0.0
    epoch_steps = 0

    progress_bar = tqdm(dataloader, desc=f"Epoch {epoch}/{EPOCHS}")

    for step, batch in enumerate(progress_bar):
        # å‰å‘ä¼ æ’­
        input_ids = batch["input_ids"]
        attention_mask = batch["attention_mask"]
        pixel_values = batch["pixel_values"].to(dtype=weight_dtype)

        with torch.no_grad():
            encoder_hidden_states = text_encoder(input_ids, attention_mask)[0]
            latents = vae.encode(pixel_values).latent_dist.sample()
            latents = latents * vae.config.scaling_factor

        # æ·»åŠ å™ªå£°
        noise = torch.randn_like(latents, dtype=weight_dtype)
        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps,
                                  (latents.shape[0],), device=latents.device).long()
        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)

        # UNeté¢„æµ‹
        model_pred = unet(noisy_latents, timesteps, encoder_hidden_states, return_dict=False)[0]

        # è®¡ç®—æŸå¤±
        if noise_scheduler.config.prediction_type == "epsilon":
            target = noise
        else:
            target = noise_scheduler.get_velocity(latents, noise, timesteps)

        loss = torch.nn.functional.mse_loss(model_pred.float(), target.float(), reduction="mean")

        # åå‘ä¼ æ’­
        accelerator.backward(loss)

        # æ¢¯åº¦æ£€æŸ¥
        if global_step % 10 == 0:
            grad_status = check_gradients(unet, global_step)
            gradient_health_history.append((global_step, grad_status))

            # è‡ªåŠ¨è°ƒæ•´å­¦ä¹ ç‡
            if grad_status == "critical":
                for param_group in optimizer.param_groups:
                    old_lr = param_group['lr']
                    param_group['lr'] *= 2.0
                    print(f"æé«˜å­¦ä¹ ç‡: {old_lr:.2e} â†’ {param_group['lr']:.2e}")

        # ä¼˜åŒ–
        if accelerator.sync_gradients:
            accelerator.clip_grad_norm_(lora_layers, MAX_GRAD_NORM)

        optimizer.step()
        optimizer.zero_grad()

        # è®°å½•
        current_loss = loss.detach().item()
        total_loss += current_loss
        step_losses.append(current_loss)
        epoch_steps += 1
        global_step += 1

        progress_bar.set_postfix({
            "loss": f"{current_loss:.4f}",
            "lr": f"{optimizer.param_groups[0]['lr']:.2e}",
        })

        # ä¿å­˜æ£€æŸ¥ç‚¹
        if global_step % SAVE_STEPS == 0:
            checkpoint_path = os.path.join(CHECKPOINT_DIR, f"emergency_{global_step}")
            os.makedirs(checkpoint_path, exist_ok=True)

            unwrapped_unet = accelerator.unwrap_model(unet)
            lora_state_dict = convert_state_dict_to_diffusers(get_peft_model_state_dict(unwrapped_unet))

            StableDiffusionPipeline.save_lora_weights(
                save_directory=checkpoint_path,
                unet_lora_layers=lora_state_dict,
                safe_serialization=False,
            )

    # Epochç»“æŸ
    avg_loss = total_loss / epoch_steps
    epoch_losses.append(avg_loss)

    if avg_loss < best_loss:
        best_loss = avg_loss
        print(f"\næ–°çš„æœ€ä½³loss: {best_loss:.6f}")

    # è®¡ç®—å˜åŒ–
    loss_change_pct = 0
    if len(epoch_losses) > 1:
        loss_change = epoch_losses[-2] - avg_loss
        loss_change_pct = (loss_change / epoch_losses[-2]) * 100

    print(f"\nEpoch {epoch} å®Œæˆ:")
    print(f"  å¹³å‡Loss: {avg_loss:.6f}")
    print(f"  å˜åŒ–: {loss_change_pct:+.2f}%")
    print(f"  å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.2e}")

print("è®­ç»ƒå®Œæˆ!")

# ä¿å­˜æœ€ç»ˆæ¨¡å‹
print("ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
unwrapped_unet = accelerator.unwrap_model(unet)
lora_state_dict = convert_state_dict_to_diffusers(get_peft_model_state_dict(unwrapped_unet))

StableDiffusionPipeline.save_lora_weights(
    save_directory=OUTPUT_DIR,
    unet_lora_layers=lora_state_dict,
    safe_serialization=False,
)

# ç”ŸæˆæŠ¥å‘Š
report = {
    "training_completed": True,
    "epochs": len(epoch_losses),
    "final_loss": best_loss,
    "learning_rate": optimizer.param_groups[0]['lr'],
    "gradient_checks": len(gradient_health_history),
    "healthy_gradients": sum(1 for _, status in gradient_health_history if status == "healthy")
}

with open(os.path.join(OUTPUT_DIR, "emergency_report.json"), "w") as f:
    json.dump(report, f, indent=2)

# ç®€å•å¯è§†åŒ–
if len(step_losses) > 0:
    plt.figure(figsize=(12, 4))

    plt.subplot(1, 2, 1)
    plt.plot(step_losses)
    plt.title("Loss Curve")
    plt.xlabel("Step")
    plt.ylabel("Loss")

    plt.subplot(1, 2, 2)
    if gradient_health_history:
        steps, statuses = zip(*gradient_health_history)
        status_values = [0 if s == "critical" else 1 if s == "too_small" else 2 for s in statuses]
        plt.scatter(steps, status_values)
        plt.yticks([0, 1, 2], ['Critical', 'Too Small', 'Healthy'])
        plt.title("Gradient Health")
        plt.xlabel("Step")

    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, "training_summary.png"))
    plt.show()

print(f"âœ… ç´§æ€¥ä¿®å¤å®Œæˆ!")
print(f"æ¨¡å‹ä¿å­˜åœ¨: {OUTPUT_DIR}")
print(f"æœ€ç»ˆloss: {best_loss:.6f}")

# æ¢¯åº¦å¥åº·æ€»ç»“
if gradient_health_history:
    healthy_count = sum(1 for _, status in gradient_health_history if status == "healthy")
    total_checks = len(gradient_health_history)
    print(f"æ¢¯åº¦å¥åº·æ£€æŸ¥: {healthy_count}/{total_checks} ({healthy_count / total_checks * 100:.1f}%)")

    if healthy_count > 0:
        print("âœ… æ£€æµ‹åˆ°å¥åº·æ¢¯åº¦ï¼Œä¿®å¤æˆåŠŸ!")
    else:
        print("âŒ ä»æœªæ£€æµ‹åˆ°å¥åº·æ¢¯åº¦ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")