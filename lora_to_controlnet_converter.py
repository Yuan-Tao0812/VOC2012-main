"""
å°†è®­ç»ƒå¥½çš„LoRAæƒé‡åˆå¹¶åˆ°SDæ¨¡å‹ä¸­ï¼Œå¹¶ä¿å­˜ä¸ºControlNetè®­ç»ƒæ‰€éœ€çš„æ ¼å¼
"""

import torch
import os
from diffusers import StableDiffusionPipeline
from safetensors.torch import save_file, load_file


def merge_lora_to_full_model(
        base_model_path="stable-diffusion-v1-5/stable-diffusion-v1-5",
        lora_weights_path="/content/drive/MyDrive/VisDrone2019-YOLO/trained_lora_controlnet/",
        output_path="/content/drive/MyDrive/VisDrone2019-YOLO/sd15_with_lora.ckpt",
        lora_scale=1.0
):
    """
    å°†LoRAæƒé‡åˆå¹¶åˆ°å®Œæ•´çš„SDæ¨¡å‹ä¸­

    Args:
        base_model_path: åŸºç¡€SDæ¨¡å‹è·¯å¾„
        lora_weights_path: LoRAæƒé‡ç›®å½•
        output_path: è¾“å‡ºçš„å®Œæ•´æ¨¡å‹è·¯å¾„
        lora_scale: LoRAæƒé‡ç¼©æ”¾ç³»æ•°
    """

    print("ğŸ”„ å¼€å§‹åˆå¹¶LoRAæƒé‡åˆ°å®Œæ•´æ¨¡å‹...")

    # 1. åŠ è½½åŸºç¡€æ¨¡å‹
    print("ğŸ“¥ åŠ è½½åŸºç¡€SDæ¨¡å‹...")
    pipeline = StableDiffusionPipeline.from_pretrained(
        base_model_path,
        torch_dtype=torch.float32,  # ä½¿ç”¨float32ç¡®ä¿ç²¾åº¦
        safety_checker=None,
        requires_safety_checker=False
    )

    # 2. åŠ è½½LoRAæƒé‡
    print("ğŸ“¥ åŠ è½½LoRAæƒé‡...")
    pipeline.load_lora_weights(lora_weights_path)

    # 3. è®¾ç½®LoRAç¼©æ”¾ç³»æ•°
    if hasattr(pipeline, 'set_adapters'):
        pipeline.set_adapters(["default"], [lora_scale])

    # 4. åˆå¹¶LoRAæƒé‡åˆ°UNet
    print("ğŸ”— åˆå¹¶LoRAæƒé‡...")
    pipeline.fuse_lora(lora_scale=lora_scale)

    # 5. æ„å»ºå®Œæ•´çš„state_dict
    print("ğŸ’¾ æ„å»ºå®Œæ•´æ¨¡å‹state_dict...")

    # è·å–å„ä¸ªç»„ä»¶çš„state_dict
    unet_state_dict = pipeline.unet.state_dict()
    vae_state_dict = pipeline.vae.state_dict()
    text_encoder_state_dict = pipeline.text_encoder.state_dict()

    # æ„å»ºå®Œæ•´çš„æ¨¡å‹state_dictï¼ˆControlNetæ ¼å¼ï¼‰
    full_state_dict = {}

    # UNetæƒé‡ - æ·»åŠ "model.diffusion_model."å‰ç¼€
    for key, value in unet_state_dict.items():
        new_key = f"model.diffusion_model.{key}"
        full_state_dict[new_key] = value

    # VAEæƒé‡
    # Decoder
    for key, value in vae_state_dict.items():
        if key.startswith("decoder."):
            new_key = f"first_stage_model.{key}"
            full_state_dict[new_key] = value
        elif key.startswith("encoder."):
            new_key = f"first_stage_model.{key}"
            full_state_dict[new_key] = value
        elif key.startswith("quant_conv."):
            new_key = f"first_stage_model.{key}"
            full_state_dict[new_key] = value
        elif key.startswith("post_quant_conv."):
            new_key = f"first_stage_model.{key}"
            full_state_dict[new_key] = value

    # Text Encoderæƒé‡
    for key, value in text_encoder_state_dict.items():
        new_key = f"cond_stage_model.transformer.{key}"
        full_state_dict[new_key] = value

    # 6. ä¿å­˜ä¸º.ckptæ ¼å¼
    print(f"ğŸ’¾ ä¿å­˜å®Œæ•´æ¨¡å‹åˆ°: {output_path}")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    # ä¿å­˜ä¸º.ckptæ ¼å¼
    torch.save({
        'state_dict': full_state_dict,
        'global_step': 0,
        'epoch': 0,
    }, output_path)

    print("âœ… åˆå¹¶å®Œæˆï¼")

    # 7. éªŒè¯ä¿å­˜çš„æ¨¡å‹
    print("ğŸ” éªŒè¯ä¿å­˜çš„æ¨¡å‹...")
    try:
        checkpoint = torch.load(output_path, map_location="cpu")
        state_dict = checkpoint['state_dict']

        print(f"ğŸ“Š æ¨¡å‹ç»Ÿè®¡:")
        print(f"  - æ€»å‚æ•°æ•°é‡: {len(state_dict)}")
        print(f"  - UNetå‚æ•°: {len([k for k in state_dict.keys() if 'diffusion_model' in k])}")
        print(f"  - VAEå‚æ•°: {len([k for k in state_dict.keys() if 'first_stage_model' in k])}")
        print(f"  - TextEncoderå‚æ•°: {len([k for k in state_dict.keys() if 'cond_stage_model' in k])}")
        print(f"  - æ–‡ä»¶å¤§å°: {os.path.getsize(output_path) / 1024 / 1024 / 1024:.2f} GB")

        print("âœ… æ¨¡å‹éªŒè¯é€šè¿‡ï¼")

    except Exception as e:
        print(f"âŒ æ¨¡å‹éªŒè¯å¤±è´¥: {e}")

    # 8. æ¸…ç†å†…å­˜
    del pipeline
    torch.cuda.empty_cache()

    return output_path


def create_controlnet_config():
    """
    åˆ›å»ºControlNetè®­ç»ƒæ‰€éœ€çš„é…ç½®æ–‡ä»¶
    """
    config_content = """
# ControlNeté…ç½®æ–‡ä»¶
model:
  base_learning_rate: 1.0e-5
  target: cldm.cldm.ControlLDM
  params:
    linear_start: 0.00085
    linear_end: 0.0120
    num_timesteps_cond: 1
    log_every_t: 200
    timesteps: 1000
    first_stage_key: "jpg"
    cond_stage_key: "txt"
    control_key: "hint"
    image_size: 64
    channels: 4
    cond_stage_trainable: false
    conditioning_key: crossattn
    monitor: val/loss_simple_ema
    scale_factor: 0.18215
    use_ema: False
    only_mid_control: False

    control_stage_config:
      target: cldm.controlnet.ControlNet
      params:
        image_size: 32 # unused
        in_channels: 4
        model_channels: 320
        attention_resolutions: [ 4, 2, 1 ]
        num_res_blocks: 2
        channel_mult: [ 1, 2, 4, 4 ]
        num_heads: 8
        use_spatial_transformer: True
        transformer_depth: 1
        context_dim: 768
        use_checkpoint: True
        legacy: False

    unet_config:
      target: cldm.cldm.ControlledUnetModel
      params:
        image_size: 32 # unused
        in_channels: 4
        out_channels: 4
        model_channels: 320
        attention_resolutions: [ 4, 2, 1 ]
        num_res_blocks: 2
        channel_mult: [ 1, 2, 4, 4 ]
        num_heads: 8
        use_spatial_transformer: True
        transformer_depth: 1
        context_dim: 768
        use_checkpoint: True
        legacy: False

    first_stage_config:
      target: ldm.models.autoencoder.AutoencoderKL
      params:
        embed_dim: 4
        monitor: val/rec_loss
        ddconfig:
          double_z: true
          z_channels: 4
          resolution: 256
          in_channels: 3
          out_channels: 3
          ch: 128
          ch_mult:
          - 1
          - 2
          - 4
          - 4
          num_res_blocks: 2
          attn_resolutions: []
          dropout: 0.0
        lossconfig:
          target: torch.nn.Identity

    cond_stage_config:
      target: ldm.modules.encoders.modules.FrozenCLIPEmbedder
"""

    return config_content


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # è½¬æ¢LoRAæ¨¡å‹
    merged_model_path = merge_lora_to_full_model(
        base_model_path="stable-diffusion-v1-5/stable-diffusion-v1-5",
        lora_weights_path="/content/drive/MyDrive/VisDrone2019-YOLO/trained_lora_controlnet/",
        output_path="/content/drive/MyDrive/VisDrone2019-YOLO/sd15_with_lora.ckpt",
        lora_scale=1.0
    )

    # ä¿å­˜é…ç½®æ–‡ä»¶
    config_content = create_controlnet_config()
    config_path = "/content/drive/MyDrive/VisDrone2019-YOLO/controlnet_config.yaml"

    with open(config_path, "w") as f:
        f.write(config_content)

    print(f"\nğŸ¯ è½¬æ¢å®Œæˆï¼ç°åœ¨ä½ å¯ä»¥ç”¨äºControlNetè®­ç»ƒ:")
    print(f"ğŸ“ åˆå¹¶åçš„æ¨¡å‹: {merged_model_path}")
    print(f"ğŸ“ é…ç½®æ–‡ä»¶: {config_path}")

    print(f"\nğŸ”§ ControlNetè®­ç»ƒä»£ç ä¿®æ”¹:")
    print(f"resume_path = '{merged_model_path}'")
    print(f"# å°† './models/control_sd15_ini.ckpt' æ”¹ä¸ºä¸Šé¢çš„è·¯å¾„")